So, machen wir mal ein bisschen weiter.
Also Leute, also irgendwie kann mein Vorschau-Rechner den Emoji im Stream-Titel nicht, aber das
kann doch eigentlich mittlerweile jeder, was ist das denn, Unicode 05, nee, der kann doch
ja, keine Ahnung, wer Schmerzen hat, wahrscheinlich spackt der Browser einfach rum, aber war's
nicht, aber war's nicht.
Den haben wir am Start, der Captain Siekay ist wieder da, huge, Jettüftelfelix, nice,
so, ja ich zeige euch jetzt erstmal, wo wir stehen, was gerade aktuell ist, beziehungsweise
nee, ich fang mal anders an, also, ich erzähle erstmal, was wir heute machen, wenn ein paar
Leute am Start sind, übrigens den ganzen Krämpel von heute Morgen kann ich eigentlich mal
hier zumachen.
Lul.
Und ich muss eine Maus anschließen an den Rechner, äh, am zweiten Rechner wohlgemerkt,
also der hier fürs Streaming, spackt gerade übelst rum, naja, man war's nicht, achso weil
heute Morgen die Frage kam, was für NFC-Tags ich hierfür verwende, habe ich erst später
gelesen, muss schon weg, also ja, ich habe heute Morgen ein Video gezeigt von meinem
Bart, wo ich zeige, wie ich hier mit dem iPad über den NFC-Tag, hier über den NFC-Tag
meinen Heizer einschalte, hier so zack, dranhalten, und dann geht der Lüfter an, und dann zeige
ich noch irgendwo die Steckdose, wie das Ganze eingeschaltet wird und sowas, aber heute
Morgen hat irgendjemand gefragt, was ich da für NFC-Tags verwende, kann ich euch zeigen,
und zwar, Postchamp, Postchamp, Postchamp, ich muss es mal kurz hier aufmachen, äh, ich
habe mir so ein NFC-Tag-Set gekauft, und letztendlich, ihr könnt wirklich alles mögliche für NFC-Tags
verwenden, also, das Gute ist, es ist eigentlich noch viel zu teuer für das was, ne, das Gute
an NFC-Tags ist, ihr könnt die auch reinweise kaputt machen, der Kram ist echt abgrundtief
billig, also billig meine ich jetzt, also günstig, NFC-Tags sind wirklich so unglaublich kosteneffizient,
wenn man was machen will, also, wenn man die Auswahl hat, macht man einen Knopf irgendwo
hin, oder hat man sein Handy dabei und macht einen NFC-Tag hin, macht einen NFC-Tag hin,
ein NFC-Tag geht wahrscheinlich weniger kaputt, und ein NFC-Tag kostet nix, wirklich gar nix,
wenn du hier guckst, die billigen NFC-Tags kostet 2 Centenstücke oder so, das ist wirklich
massiv billig, und, ähm, wie viel Speicher, boah, keine Ahnung, 120 byte oder so, steht
das hier irgendwo, dann zeige ich euch was wir heute machen, äh, steht hier irgendwo
bytes, Speicher, bytes, bytes, 888 bytes Speicher, ok, hat der eine, also, es ist nicht viel,
also du hast weniger als ein Kilobyte Speicher, du hast paar, paar bytes, also du hast echt
nicht viel Speicher, so ein NFC-Tag, aber brauchst du letztendlich auch nicht, ja, das
wird, ich muss mal kurz den Chat aufmachen, also das ist eines der wenigen Dinger heutzutage,
der noch kein Doom laufen lassen kann, also Doom auf dem NFC-Tag geht noch nicht, man
muss aber ja auch sagen, ich weiß ja natürlich, das ist ja ein Gag, aber die Frage ist gar
nicht mal so unberechtigt, wenn man sich anguckt, dass man heute auf Druckern, auf dem Klo und
auf einem Schwangerschaftstest, wo geht das, Schwangerschaftstest war fake, aber man kann
ja mittlerweile überall Doom drauf spielen, das heißt, warum nicht da, gut, bei dem NFC-Tag
ist es relativ einfach, die haben ja gar keine CPU in dem Sinn drauf, das ist ja nur
irgendeine Antenne und, äh, ein paar bytes Speicher, genau, aber das ist wirklich top,
also kann man nichts sagen, auch dieses Set hier finde ich ganz gut, du hast magnetische
Sticker und alles mögliche, und wisst ihr was auch richtig nice ist, was ich mir jetzt
auch noch einen Schalter bestellt habe, und zwar einen, wobei man den sich auf Banggood
kaufen muss, zeig den mal hier, weil da ist er billiger, Banggood, Alter, das ist die
Seite mit dem Abstand besten Namen, wer kauft noch was bei Banggood, und zwar ihr müsst
an Aquara, Opel oder so heißt das, total bescheuerte Name, ne, doch den hier, hier also, die gibt
es teilweise recht günstig, aus China, was, wo ist das aus China, what the fuck, das kriegst
ja fast schon, ihr müsst mal gucken, wo es das gibt, das sind Schalter, das sind ZigPee-Schalter,
und die kosten, wenn man da ordentlich, ordentlich guckt, irgendwie, keine Ahnung, 10 bis 15
Dollar, so das Stück, gibt es bis zu 3 Schalter, und die sind richtig cool gemacht, also erst
ist kann man die pairen, total easy, mit jedem x-beliebigen ZigPee-Stick und ZigPee-Hub und
sonst was, und das coole ist, man kann den Schalter rausnehmen, man kann das so als,
so Art Fernbedienung verwenden, und die Rückseite ist magnetisch, das heißt man kann den einfach
an den Türrahmen hängen, und ich hab jetzt hier einfach an zwei Türrahmen so ein Ding
gesteckt, und da kann man jetzt Sachen mit einmal ausschalten, eigentlich richtig praktisch,
ja, aber was auch immer, übrigens guckt euch mal hier an, was ich, also meine Twitter-Medien-Historie
ist wirklich fail, die besteht jetzt im größten Teil aus irgendwelchen Emotes, und naja, so
also ich zeige euch mal, was wir heute machen, so also wo wir stehen geblieben sind, übrigens,
du bist nicht der erste, der sagt französische Werbung vorm Stream, heute Morgen hat jeder
geschrieben französische Werbung, ich weiß nicht warum es französische Werbung vorm Stream
gibt, die Pogers, Pogers, Pogers sind los, also meine Stream-Sprache steht nicht auf
französisch oder, Zuschauer alle, Stream-Sprache, Deutsch ist das deutsche Sprache hier im Stream,
aber naja, ich weiß nicht warum es da französische Werbung gibt, so, ich hab jetzt mal den aktuellen
Stand vorher gepusht, und zwar, also ich pull das jetzt mal, ich pull das jetzt mal, also
weiß ich, dann zeige ich mal, was, also was wir im letzten Stream gemacht haben, wenn
ich es euch angucken will, ich hab's ja, auf drängen des Chats letztes mal, hab ich das
Repo gepusht, obwohl ich das Repo noch überhaupt nicht pushen wollte, und dann Chat, jetzt
muss ich ja mal ein ernstes Wörtchen mit dem Chat reden, ja, Chat lacks critical information,
so, ich wollte das das letzte mal nicht pushen, und ich hab's gepusht, weil das der Chat
unbedingt wollte, und jetzt hab ich ein Stern gekriegt, also ich hab jetzt am mindestens
gerechnet, dass zumindest jetzt so Alibi-mäßig mal so 3-4 Leute wenigstens mal das Ding starren
aufgeht haben, aber nee, EINER, demnächst hör ich nicht mehr auf den Chat, hallo, ich
will nicht, ich will nicht, was auch immer, orienti-be, what the f, ich, ah, nicht docker,
ich hab mich verschrieben, lull, wenn ich das richtige schreiben will, ist das denn,
so, also pullen wir mal das aktuelle, was auch immer, so, und jetzt zeige ich euch mal,
was das Ding macht, und was die Änderungen sind seit dem letzten Mal, so, ich starte
das jetzt erst mal,.net-run,.net-run, jetzt sind's zwei, ajajaja, exzellent, so, also,
ich hab mal ein paar Fake-Webcams hinzugefügt, nämlich diesen Stream hier, so, und, achso,
jetzt hab ich's hier, ich hätt's auch hier starten können, nee, egal, so, und wenn ich
das jetzt im Browser aufmach, ich mach jetzt mal hier, Browser, irgendwie Chrome auf,
und wenn man jetzt hier auf die Seite geht, also sprich hier auf Local Host, wobei wir
gehen nicht auf Local Host, da spackt das manchmal rum, wir gehen mal hier, IP League,
ich weiß immer IP League, ganz, ganz furchtbar kritisch, Port 5000, dann sehen wir jetzt
erst mal nicht viel, wir sehen jetzt, paar Streams, so, das könnten allerdings Webcam-Streams
sein, also sprich, was dieses, was dieses Programm macht, ist folgendes, das nimmt
sich Streams von der Webcam, das nimmt sich Pakete von der Webcam, packt die ein, und
schickt die über WebRTC an den Browser, ja, Corona, so, und das funktioniert hier erstaunlich
gut und ohne zu rocken, damit man mal da ein bisschen was anderes sieht, werd ich jetzt
mal was leaken, und zwar, ich werd jetzt mal mein Hoftor leaken, aber zumindest, was das
Ziel ist, was wir heute machen, ist, paar halbwegs realistische Kamerabilder reinmachen, nicht
unbedingt diesen Teststream, gibt's einen Delay, nee, ich hab's letztes Mal im Stream
gezeigt, es gibt nahezu kein Delay, wir können das ausprobieren, wir können das mit dem
Delay noch mal ausprobieren, warte mal, ich mach mal kurz mein iPhone an, und ich zeig
euch das mal, dass es kaum ein Delay gibt, ja, wartet mal, ich mach mal kurz, ach nee,
wartet mal kurz, muss das grad noch umstellen, mein iPhone ist nämlich der Meinung, es kann
den Stream nicht anmachen, ich zeig euch mal, dass es nahezu kein Delay gibt, ich zeig's
euch erstmal in VLC, und danach zeig ich's euch mal in meinem Streamer, da werden wir
sehen, da sind Welten dazwischen, was Delay angeht, so, Kamera Access, Livestreamer an,
okay.
Also ich zeig's euch mal im VLC, ich zeig's euch mal im VLC, VLC, erstmal etwas verkleinert
falls ich was liege, man weiß ja nie, hallo, Alter warum ist so, warum ist sowas in Windows
möglich?
Kann mir das mal einer erklären?
Warum, warum kann man Fenster so verschieben, dass man sie nicht mehr zurückschieben kann?
Richtig dumm, richtig dumm, so und jetzt haben wir hier öffnen, Network Stream, ok 110.
Oh hat mein iPhone gar keine 100 IP, 110 mehr, was kann mein iPhone für eine IP haben?
Ok ich muss mal in meinen Rauder gucken, was mein Rauder sagt, ok, ich glaube mein iPhone
hat ein neuer IP gekriegt, ist ja nicht fixed, so IP, DHCP Server, Leases, iPhone, 110,
also mein iPhone hat angeblich 110, ok, kann man dem iPhone irgendwie nachgucken was
man für eine IP hat?
Warum kriegt mein iPhone nicht die reservierte, sondern so eine komische kacke IP, was ist
das denn schon wieder hier?
Weiß, da weiß man echt nicht was los ist bei Apple manchmal, so, also dann nehmen wir
jetzt mal die Verzögerung im VLC, pass mal auf, Verzögerung im VLC, ich muss mal kurz
gucken wie ich das mache, also ich klatsche da jetzt mal irgendwie so, dass ihr es hört,
also hören ist quasi, ihr seht das Klatschen zum Hören, zu dem ich die Hände zusammenmache,
ich leg mal kurz das hier hin, so, easy clap, also ich würd mal sagen, was meint ihr, wie
viel Verzögerung ist das, so 3 Sekunden oder so, 3 Sekunden irgendwie so in dem dreh, ich
spiegel mich in die Hand, OMG, 2 Sekunden, ok, jetzt pass mal auf, jetzt mach ich das
mal bei mir, in meinem Streaming Programm, ich mach mal nur eine Cam, weil ich mein was
Braume da vorbeigekomme, ich mach, das Wohnzimmer, das Wohnzimmer machen wir jetzt hier zu meinem
Stream, 1, 2, 3, 4, 1, 2, 1, 4, 6 war's, zack,.net, watch, watch, run, so jetzt passt mal auf,
wie verzu, ah, ich dab, ich hab's hier schon, ich hab's hier schon laufen, lul, jaja, Blablabla,
ist ok, so jetzt gehen wir, ich geh auch extra mal hier in Windows rein, dann sieht man das
nicht besser, mal Chrome auf, Chrome, so, 1, 2, 1, 6, 8, 2,., was war das hier, 1, 3,
1, 5000, so, also, und jetzt passt mal auf, guckt euch mal an, wie nicht existent diese
Verzögerung ist, ist das nicht Massive Poggers, wie das ist nicht genau gleich, das ist überhaupt
nicht genau gleich, das ist, ok, ich mach's jetzt nebeneinander, Leute, ich mach's jetzt
nebeneinander, ich mach, die Verzögerung ist, keine Ahnung, 100 Millisekunden oder so, wenn
es hochkommt, also das ist ja, oder 300 vielleicht, ja, das ist viel, ich weiß, ich weiß, chat,
viele Leute haben da kein Gefühl für, ja, ja, ich weiß, das ist jetzt natürlich nicht
so richtig guter Vergleich, weil, weiß gar nicht, ob man sich da mehrfach dazu connecten
kann, aber wir gucken uns das jetzt mal im Vergleich an, easy clap, kann man das wieder
richtig machen, ok, so, jetzt passt mal auf, ok, so, jetzt passt mal auf, ok, so, jetzt
passt mal auf, ok, also chat, jetzt ist es glaube ich jedem aufgefallen, dass die Verzögerung
ein bisschen höher ist, normalerweise, ja, so, und man muss dazu sagen, das ist jetzt
sogar noch gestreamt in Browser, normalerweise, wenn du so Sachen über den Browser machst,
dann ist das ja nochmal nen Tick langsamer, so, und diesmal ist es jetzt hier quasi, nicht
nur einfach schneller als im Player, es ist auch noch im Browser schneller, also das ist
massive POG, so, bringt das Laden ist eigentlich auch relativ schnell, so, ich hab mir schon
überlegt, ob ich nicht immer den ersten Keyframe einfach mitsende, ok, so, das ist das erste,
was wir hier oben am Start haben, das kann ich jetzt einfach mal da drauf liegen lassen
und das juckt jetzt ja nicht weiter, die Zeit tickt hoch, so, und jetzt leak ich nochmal
was und zwar mein Hoftor, weil da hab ich ne Testaufnahme drin, von gestern Abend, Hoftor,
ultimatives Hoftor Leak, das nehmen wir jetzt, das ist Nachtsicht von der Kamera, das ist
Nachtsicht von der Kamera und irgendwann läuft jetzt gleich jemand vorbei, so, das könnte
sein, dass eine Spinne da vorsitzt, ja, oder Spinnenweben zumindest, so, und diesen Clip
verwenden wir jetzt mal zum nächsten DSGVO, ich hab vorher gefragt, sogar wirklich, ich
film jetzt nicht einfach hier Leute und dann, so, also, man erkennt ja auch nix drauf, so,
und das nehmen wir heute zur Motion Detection Erkennungsgeschichte, ja, das bin ich nicht
selbst, nein, selbst wenn ist es eigentlich vollkommen egal, wer das ist, so, jetzt muss
ich den Kram nur nochmal in die VM kopieren kurz, Desktop, wie hab ich es genannt, Fluor,
ja das heißt Fluor 2, total bescheuert, das hat mit Fluor überhaupt nix zu tun, so, exellent,
jetzt mit Google Street View, jaja macht ihr mal, so, ich glaube, jetzt hat man das ganz
gut erkannt zumindest, also sprich, was wir vorhaben, also sprich, die Kram mit die Verzögerung
ist übelst gering da drüber, so, und was ich jetzt vorhab' ist, wir nehmen jetzt diesen
Clip hier, Moment, wo hab ich denn überhaupt hinkopiert, ich glaube in den Zaum, der VLC
Fluor 2, genau, so, wir nehmen diesen Clip und da machen wir jetzt Object Detection und
zwar hab ich mir das folgendermaßen überlegt, Object Detection selbst braucht viel zu viel,
Moment, das ist nicht alles, was der kopiert hat, oder, der hat nicht alle, what the fuck,
der hat nicht alles kopiert, oder, ne, das geht nochmal nach 47 Sekunden, ja, seh'
mal, der kommt gar nicht da, jetzt ist es ok, jetzt ist es da, so, jetzt ist es da, exellent,
ok, so, und die Sache ist die, das braucht ganz schön viel, ÖZOfficial, ich, was verwendest
du für den, mein OpenCV selbst kann ja nur die Geschichte mit Pixel, die sich ändern
und so erkennen, das braucht ja dann meistens noch irgendwelche Trainingsmodelle und sowas
unten drunter, dass es mit Object Detection gut funktioniert, oder was verwendest du,
was nimmst du, da, achso, Kantenerkennung, ja, das ist auch eine gute Idee, das haben
wir sogar im Stream schon gemacht, so, also, was ich machen will ist folgendes, also, erstens,
das verbraucht, hier Object Detection machen, verbraucht ganz schön viel Leistung, sprich,
wenn du jeden Frame analysieren willst, da kommt ja sogar, wahrscheinlich, naja, gut,
ne, also, mein Desktop kommt da nicht unbedingt bei ins Schwitzen, aber ein Raspberry Pi schafft
das nicht, keine Chance auf ein Raspberry Pi, das mit jedem Frame zu machen, deswegen
hab ich mir folgendes überlegt, scheiß drauf, jeden Frame, brauchen wir gar nicht, wir nehmen
einfach jeden Keyframe, wir nehmen jeden Keyframe von der Kamera, alle zwei Sekunden, sag mal
was, was schneller eintritt, so, entweder jeden Keyframe, oder wenn die Keyframes schneller
kommen als alle zwei Sekunden, dann nicht alle zwei Sekunden, so, ein Keyframe, nur mal
zur Erklärung, was ein Keyframe in einem Video ist, es gibt grundsätzlich zwei verschiedene
paar Frames in einem Video, das ist, also, komplett anders, als jetzt im Vergleich zu
irgendwelchen Spielen, die gerendert werden, oder so, also, bei einem Spiel, oder generell
bei allem hier so, was ihr da so seht, wird ja immer das komplette Bild berechnet, und
bei Videos ist das grundsätzlich so, es gibt eigentlich erst mal nur zwei, es gibt natürlich
noch andere Varianten, aber so grundsätzlich zwei verschiedene Sachen, es gibt Keyframes,
das ist immer ein komplettes Bild, und es gibt, sag mal, nicht Keyframes, das ist immer
der Unterschied zum letzten Keyframe, beziehungsweise der Unterschied zum letzten Frame, zum letzten
Keyframe und so was, also sprich, es gibt einfach Keyframes, da ist komplettes Bild
immer komplett, und es gibt die anderen Frames, ich glaube iFrames nennt sich das dann, das
ist immer nur ein Diff, was natürlich den riesen Vorteil hat, man spart unglaublich
viel Speicherplatz, sprich, ich muss, wenn ich alle zwei Sekunden nen Keyframe mach, manche
Kameras machen sogar nur alle 10 Sekunden nen Keyframe, dann spar ich natürlich unglaublich
viel Pixel, vor allem bei ner Webcam oder bei ner Kamera, wo sich nicht so viel bewegt,
da ist dann, die normalen Frames sind dann halt nicht vorhanden, weil sich das Bild nicht
ändert, also das hier sind ein paar Kilobit oder so, die die an Informationen speichern
muss, weil es bewegt sich ja kaum was.
Es gibt iP und bFrames, ja der Chat ist da sicherlich besser da drin, das zu erklären
als ich, ich weiß nicht, was es noch für ne dritte Sorte Frames gibt, aber das sind
so die zwei grundsätzlichen Unterschiede, ich glaub iFrames sind die Sachen, die dazwischen
liegen, bFrames sind glaub ich die Keyframes, ich bin aber gar nicht sicher, zumindest,
wie sind die Keyframes im Stream markiert, die haben spezielle Fleck gesetzt, dass man
weiß, was die Keyframes sind, das lese ich ja hier übrigens auch aus, das zeig ich euch
mal.
Oh, ich glaub, wohnen wir gebannt, das hat grad die Verbindung zum Test Stream verloren.
Wohnen wir, wohnen wir, nee, wohnen wir gar nicht gebannt, dachte, ich wurd schon mal
gebannt von denen, weil ich zu oft den Stream aufgemacht hab, zumindest glaube ich, weil
ich glaub ich, dass ich gebannt wurde, ich hab da nicht zu oft drauf zugegriffen, zu
oft drauf zugegriffen.
Okay, jetzt der Chat ist BigBrain, bei H264, das ist übrigens was wir supporten, ansonsten
wir supporten nur H264, das ist auch das, was 99% aller Kameras machen, also, der iFrame
ist das gesamte Bild, dann gibt es den pFrame, der das Diff quasi zum vorherigen Bild ist
und dann gibt es was, ein bFrame, der berechnet das aus vorherigem Nachfolgen mit, exellent,
letztendlich, unter der Haube, was hängen bleiben muss ist, es gibt Frames, die sind
komplett alles, also komplett, jedes mal ein volles Bild und es gibt Frames, das sind nur
die Unterschiede zu entweder letztem Keyframe oder zunächstem Bild, also einmal werden nur
die Unterschiede gespeichert, einmal werden komplette Keyframes gespeichert, so, das heißt,
wir können einfach nur jeden Keyframe nehmen, decoden und dann Motion Detection, äh, nicht
Motion Detection, ähm, Object Detection mitmachen, das probieren wir jetzt mal aus.
So, und jetzt zeige ich euch mal, dass man das hier auch erkennt, was ein Keyframe ist
und was nicht, und zwar, gehen wir mal hier nach FFmpeg, ich hab das übrigens ein bisschen
umgebaut, also wer das noch vom letzten Mal kennt, ich hab das jetzt hier ein bisschen
aufgesplittet, aber es hat sich relativ viel getan, also ich hab da recht viel umgebaut,
also das hier ist das letzte, letzte Commit hier, kurz nach dem Stream und ich hab das
Ganze jetzt ein bisschen weniger Dirty gemacht, ein bisschen ordentlicher, also ich gehe
wieder nur mal kurz drüber, was sich geändert hat, ähm, ich hab jetzt was eingebaut, wo
man abfragen kann, welche Kameras es gibt, aus dem Frontend, dann hab ich diese ganze
FFmpeg-Geschichte, die mir im Startup drinnen stehen hatten, hab ich so eine eigene FFmpeg-Geschichte
draus gemacht, die ist dafür zuständig, von der Kameras die Sachen einzulesen, können
wir uns gleich angucken, ähm, ja, ein paar Helper-Klasses, hier eine, eine Klasse, die
dafür sorgt, dass FFmpeg-Sachen immer aufgeräumt werden, weil das neigt dazu ein bisschen zu
Memory-Leaken, wenn ich das nicht ordentlich aufräume, sprich jedes Mal, wenn diese Klasse
hier out of Scope geht, oder letzte Referenz weg ist, dann löscht er auch den Buffer für
ein Bild, so, ähm, dann hab ich was gebaut, wo man Sachen resize-en kann, weil das brauchen
wir auf jeden Fall, diese ganzen Machine-Learning-Dinger, die füttert man nur mit übelst Low-Res-Bildern,
also wer jetzt denkt, wir schieben da ein 1080p-Bild rein, so ein, äh, Object Detection-Algorithmus,
äh, ne, das ist meistens, keine Ahnung, 250x256x300x300, die Bilder sind übelst klein, das heißt,
wir müssen das auf jeden Fall resize-en, vorher, oder es dauerte ewig, ja, so, bla bla, so,
dann ist der Verbindungsaufbau, geht ein bisschen anders, das müssen wir uns jetzt nicht im
Detail angucken, ähm, was haben wir sonst noch, eigentlich nix als zu spannendes, genau,
und wie man rauskriegt, was Keyframes sind, ist folgendes, mit FFmpeg, kann man das Ganze
abfragen, bist du Max FPS, liegt, würde ich sagen, revealed, exposed, ja, bin ich, du
hast es richtig erkannt, so, ähm, so, und FFmpeg, äh,
blöd, warum möchte es entstehen, ist da gar nichts drinne mit Keyframe, ich hab hier
auch ein Keyframe, ach, hier ist es doch, ich bin blind manchmal, ne, also, ähm, FFmpeg,
wenn es, ähm, die Sachen von der Kamera einliest, die kann auch die Flex einlesen und man kann
quasi so abfragen, ob das, ob das jeweilige Bild, was ich eingelesen hab, ob das ein Keyframe
ist, wohlgemerkt, was ich hier einlese, ist das noch encodete Bild, also sprich, wenn
ich das Ding hier speichere, irgendwo auf der Festplatte, da kann ich damit nix anfangen,
ne, also wenn ich da jetzt, sagen wir, keine Ahnung, File, Dot, ich mach das jetzt nochmal,
da kommt jetzt nix raus, so dass ihr seht, dass es auch nicht, äh, nicht funktioniert,
ach so, Moment, wir schreiben jetzt mal hier, Home, Max, Null, Punkt, ja, gibt's ja nicht
mal ein Format, was der Corsage angeben kann, so, und dann schreiben wir hier, so, ich zeig
jetzt so mal, dass da nix Sinnvolles bei rauskommt, wenn ich diesen Frame hier einfach speichere,
es sei denn, es sei denn, FFmpeg ist übelst big brain, ne, da geht nix, keine Chance,
na, weil, das ist ein encodeter Frame, da ist noch keinerlei RGB oder irgendwelche sonstigen
Sachen drinne, keine Pixel, das ist quasi noch das komprimierte Bild, was da drin ist,
d.h. wir müssen jetzt auch erstmal noch decompressen und, also, also, die, die, nicht was decompressen,
decoden, genau, also, folgendes, was ich vor hab, ist, ich hab hier dieses Projekt auf
GitHub gefunden, was eigentlich einen ganz guten Eindruck macht, wenn ich duels, warum
auch immer, das ist ein Docker-Container mit einer recht simplen, äh, JSON-RP, so und
was man da macht, wir testen den Container auch gleich mal mit nem Curl-Aufruf, so und
was man machen kann, ist, man übergibt, hier, äh, man macht nen, nen Postrequest und schickt
dahin, ok, hier hast du meine Bild, hier hast du meine, äh, Base64-encodeden Bildinformation,
dann welchen, welche Erkennungsgeschichte man verwenden will und wie sicher er sich sein
muss. Das ist ja, das ist lustig, allerdings Cheat95 juckt mich nicht wirklich, weil ich
muss mich da null drum kümmern, ich mach einfach nen Container an und gut ist. Und ja, das
Gute ist, ich muss mich null selbst drum kümmern, um da jetzt wirklich irgendwie, äh, die,
die Machine-Learning-Sachen zu programmieren. Bis ich da was habe, was so gut funktioniert
wie das hier, ist wahrscheinlich eine Wissenschaft für sich. So, dem Ding übergibt man sein
Bild und dann gibt, gibt man ne Antwort, wo drinnen steht, äh, ob er was erkannt hat,
zum Beispiel, er ist sich zu 87%, sicher, er hat ne Person erkannt und zwar hier, ich
weiß nicht, sind das Prozentangaben, 0% links, bei Pixel können es nicht sein, von 0 bis
1, sind wahrscheinlich Prozentangaben, also quasi, 0 pro, also überhaupt nicht links,
äh, überhaupt nicht oben, äh, 0,05 pro, ah ne Moment, das muss ich mal 100 rechnen,
0% 5% links, 85% unten und 95% rechts, also, also irgendwo rechts unten auf dem Bild war
die Person. Die 4 Ecken, aber in Prozent, in Prozent. Was richtig gut ist, dass das
in Prozent ist, ne, wisst ihr das? Da muss ich nämlich keine Pixel umrechnen, das kann
ich 1 zu 1 nehmen, um irgendeinen Overlay im Frontend zu malen. Ok, wir machen jetzt
ein bisschen Big Brain Time. Erstmal starten wir den Docker-Container und probieren das
mal aus, mit der Bilderkennung. Da muss ich dazu sagen, chat, ich jibale euch, ich hab
schon ausprobiert, also ich weiß, dass es funktioniert, sonst würde ich das, würde
ich das Projekt hier gar nicht jetzt angefangen. Aber ich kann, wenn es euch lieber ist, wenn
es euch lieber ist, kann ich so tun wie, oh, ob das jetzt funktioniert, wir werden es,
wir werden es gleich erfahren, ja. Also mal gucken, ich bin mir da nicht sicher, ob es
funktioniert, also, wenn wir Glück haben, klappt das. So, also, ich muss erstmal den
Docker-Container starten. Chat, ich hab euch übrigens auf diese Art und Weise schon öfters
mal jibated und ihr habt's überhaupt nicht mitgekriegt. Ihab911, Dankeschön für den
Sub, huge, Subscription. Fake Content, ja, so, ganz, ganz schlimm. Also, Docker, haben
die da irgendwie ne Anleitung drin, wie man startet? Ja, da, genau. Äh, ja, das da.
Ja, eins muss ich machen bei diesem Docker-Container, witzigerweise. Ich muss ihm sagen, weil über
den Docker-Proxy ging es nicht, ich weiß nicht, woran das liegt. Host-Network, warum auch
immer. Dich hab ich nicht jibated, fuck, Alter. Jetzt ist es raus. Hast du heute schon dein
Spezial gezeigt? Ach so, jetzt wo du's sagst. Also, Seron möchte, möchte ich, glaube ich,
das mal wieder sehen. Hab ich euch schon mal gezeigt, wie ich meine Heizung überwache.
Ja, guckt euch das mal an. Ich kann's nicht mehr ernst bringen, Leute, das funktioniert
nicht mehr. Also, für die Leute, die neu sind, das ist so der Running-Gag in jedem Stream
und ich kann's, ich krieg's nicht mehr ernst hin. Ich hab mal fünf Streams im Folge quasi
jeden Stream erzählt, guck mal, wie geil ich meine Heizung überwache und das ist jetzt
so ein bisschen der Running-Gag. Aber guck mal, wie schön ich meine Heizung überwache.
Hier sieht man, dass der Status okay ist, weil die Kessel-Temperatur ist, auch die Pumpe
pumpt, Abgaswerte und sonst was. Aber wisst ihr was, was noch viel nicer ist? Gehen wir
mal Dashboards kurz. Ach, hier, Manage, genau. Plattenplatz nicht, Blackbox. Ich hab mir
noch was gebaut, wo ich sehe, wann meine Zertifikate auslaufen. Und ich sehe, dass
Wiki läuft in 35 Tagen aus. Anzeige ist raus. Aber das Gute ist, ich muss überhaupt nichts
machen. In fünf Tagen wird Let's Encrypt oder beziehungsweise nicht Let's Encrypt Traffic
wird das Ding automatisch selbst erneuern. Gut, also, hier läuft das Object Detection
Ding im Container. Jetzt probieren wir das mal aus. Wir brauchen jetzt erstmal ein Bild,
wo offensichtlich eine Person drauf ist. Da schnappen wir uns jetzt mal Google irgendwas
und dann machen wir hier so diesen Curl-Befehl, um das Ganze auszuprobieren. Also, ich hab
Angst nach Person zu gucken. Person. Gibt's da vielleicht irgendwie On-Couch oder so?
On-Couch. Okay, wunderbar. So, was nehmen wir da jetzt? Irgendwas, vollkommen wurscht,
so irgendwie. Den Typ mit der kurzen Hose. Ich hab Angst, immer noch sowas zu suchen.
Toxy, 10 Monate. Dankeschön. Willen Sub. Massive Subscription. Ja, ich hab jetzt einfach
mal, okay, wir nehmen jetzt irgendeinen Bild von irgendeinem Dude, der ja auf der... Handsome
Young Man Sitting. Exzellent. Gucken wir mal, ob das Motion, ob das Object Detection das
trotz Wassermarks erkennt, wahrscheinlich. View Image. Save. Handsome Young Man at Home
Sitting on the Couch. Alles klar. Natürlich. Wir beachten nur die Lizenz. Immer. Immer.
Ich mach nix anderes wie Lizenzen beachten. So, nennen wir das mal irgendwie keiner. Perz.jpg.
Exzellent. So, was wir jetzt mal machen müssen, wir müssen das Ding erst mal resizeen, damit
man das auch ordentlich ausprobieren kann. Wenn ich jetzt so wüsste, wie man Image Magic
schön resize, ich glaub resize. Ah, wunderschöne Seite. Exquisite. Ja, minusminus resize war's.
Minusminus resize auf, wir machen mal irgendwie, keine Ahnung, 300. 300 mal 300? Aber ich
weiß, dass das intern mit 300 mal 300 funktioniert, deswegen mach ich das jetzt auf 300 mal 300.
Wir werden unsere Frames auch auf 300 mal 300 resizeen. So. Gucken wir mal, ob das resized
hat. Jawoll. So. Und da machen wir jetzt mal Erkennung drauf, ob Dudes das erkennt als
Person. Vielleicht haben wir auch Glück, es erkennt es auch als Couch. Couch und Person.
Müssen wir mal schauen. So, One-liner. Okay. Zack. So. Okay, wir machen mal 50% Genauigkeit
runter. So, Cat. Und jetzt, wie hab ich gesagt, Person 2.jpg. Localhost 8080 Detect. Exzellent.
So. So, was hat er erkannt? Wir machen das mal nach JQ, dann erkennt man's besser. Was
hat er erkannt? Person hat er erkannt. Person und zwar 9 top, 28 left, 95 bottom und 67
right. Das kommt hin. Also sprich, das ist eher ein Bild, eher unten, unten Richtung
rechts. So. Ja. Also der hat quasi eher die Person da so erkannt. Aber er hat sie auf
jeden Fall erkannt und er ist sich ziemlich sicher. Er ist 80% sicher, dass er eine Person
erkannt hat. Exzellent. Wer in Bad Dudes auch Frauen erkannt? Ja. Aber ich will nicht nach
Mädels auf der Couch in der Google-Suche suchen, weil ihr wisst, was man da hinkriegt. Was
man da als Antwort kriegt wahrscheinlich. Übrigens, was ich erzählt hab, stimmt überhaupt
nicht. 28% left und 67% right. Das ist ziemlich genau die Mitte. Das ist quasi 30-60. Also
ich hab übrigens Mist erzählt. Patrick, das ist eigentlich safe for work. Ja. Also
das Bild kann man... Okay, Patrick, das Bild nehmen wir jetzt als Object Detection. Ich
muss mal kurz gucken, ob das wirklich safe ist. Ich meine, die haben zwar ganz schöne
Dinger in der Unterhose, aber das ist safe. Also we get. So. Convert. C10. 3.jpg. Also
ich glaube, man erkennt, was da eigentlich auf dem Bild zu sehen ist. Aber es ist nicht
TOS. Es kann nicht TOS sein. Es ist nichts Schlimmes. Ja, also man sieht nichts. So jetzt
gucken wir mal, was die Image-Erkennung daraus macht. So. Also. Gucken wir mal. Wie hab
ich gern Person 3? Oh, guck mal hier. Er hat erkannt Person, Person, Person, Person. Okay,
wie viel? 1, 2, 3, 6 Persons hat er erkannt. Okay. Der ist ziemlich gut, oder? 1, 2, 3,
4, 5, 6. Nicht schlecht. Okay, er hätte jetzt noch Person in Unterhose schreiben können.
Nicht verkehrt. Die Couch erkennt er nicht. So, wir können mal aus Spaß sagen, wenn er
sich ein bisschen unsicherer ist, was er noch so alles erkennt. Ein TV-Erkennter. Okay,
eine Couch. Er ist sich ziemlich sicher, dass er eine Couch erkennt. Person, Person, Person-Erkennter.
Couch, Couch. Okay. TV. Okay, wo immer dann TV sein soll. Also Object Detection funktioniert.
Ob er Anime-Characters erkennt, ist die Frage, ob das da drauf trainiert wurde. Wahrscheinlich
nicht. So, aber wir sorgen dafür, dass wir 60, 60% sicher wollen wir sein. 60% sicher
ist gut. Also man sieht, das funktioniert wunderbar. Confidence mal 1, ja dann erkennt
er alles mögliche. Wobei, geht eigentlich. Couch, Couch, TV, Person-Erkennter. Also der
ist gar nicht so fehleranfällig. TV, wo auch immer da ein Fernsehgerät ist, man weiß
es nicht. Ist da irgendwo ein Fernsehgerät? Da hinten das vielleicht. Vielleicht denkt
er, das da ist ein Fernsehgerät. Ich glaube Null geht nicht. Genau das gleiche. Der erkennt
das Fenster als Fernseher, das kann schon sein. Top, nee. Nee, der erkennt, der Fernseher
ist rechts, von links nach rechts und Mitte. Also von oben bis zur oberes Drittel vom Bild
und von links nach rechts ist das. Ja, genau, der erkennt das Fenster. Das ist das Fenster.
Oberes Drittel vom Bild, also sprich von oben bis zu ungefähr 30% vom Bild und dann fast
von links fast nach rechts. Das ist das Fenster. Das erkennt das Fenster. Das ist das Fenster,
was der als Fernseher erkennt. Also okay, für unsere Sachen reicht das. So, wir machen
jetzt mal kurz einen Benchmark, wie schnell der ist. Wohlgemerkt hier auf meinem Rechner.
Auf dem Raspberry Pi ist der nochmal eine Ecke langsamer. Auf dem Raspberry Pi kannst
du froh sein wahrscheinlich, wenn du pro Sekunde den Bild erkennen kannst. Also pro Kern oder
so. Pro Kern ein Bild. Irgendwie so würde ich mal drauf tippen auf dem aktuellen Raspberry
Pi. So, also dazu müssen wir erstmal, müssen wir mal ne Runde Benchmarks machen. Ah, hab
ich gar nicht gesehen. Ja, bester, bester Kommandozeilen Benchmark. Exzellent. Hyperfine. Hyper,
hyperfine. Ah ja, huge. What the fuck? Was ist das? Wollte ich nicht. Ah, das hier wollte
ich. Genau. Also es ist ein Benchmark Tool für die Shell. Oh, oh ich muss Update machen.
Ja, ja, ist gut. Mach Update. Also ihr seht, er erkennt schon ziemlich gut das Ganze. Kann
man sich nicht beschweren. Exquisite. Exzellent. Macht er das. So, dann muss man kurz Updates
machen. Gut, und mein Plan ist jetzt folgender, während er hier Updates macht, bevor wir
hier weitermachen können. Erklär ich mal was, was wir machen. Also, wir greifen uns jetzt
jeden Keyframe ab. Wir bekommen ja hier schon jeden Keyframe. Und wir bauen ne Motion Detection
Klasse oder ne Object Detection Klasse. Äh, Patrick, ich kann mir durchaus vorstellen,
dass er das auch erkennt als Person. Hast du das jetzt aus der Startup genommen? Ja. Es
gibt jetzt was eigenes. Guck das jetzt hier. Ich hab jetzt, ich hab das aus der Startup
rausgenommen, also was der, was der macht. Während er hier Updates kann ich ja mal so
ein bisschen grob erklären, wie das Ganze, wie das Ganze funktioniert. Also. Ähm. Gut,
diese Startup Klasse ist jetzt nicht sonderlich besonders. Das ist das, wie jede ASP.NET Startup
aussieht. Sprich, man sagt, was es für Controller gibt, was es für, äh, was Sachen registriert
werden sollen. Ich hab's jetzt so behelfsmäßig mal so gemacht. Ich hab hier ne Public-Liste
mit den Kameras drin, weil ich noch keine Datenbank angebunden hab. Ähm. So, und dann
startet er ein paar Dinger. Das Startup ist ziemlich langweilig. So, und das eigentlich
Interessante passiert einmal hier. Kamerastream. Es gibt einmal eine Kamerastream-Pool-Klasse.
Dort drüber startet man die einzelnen Kamerastreams, weil, ähm, ich brauch ja auch irgendwas,
was handelt, wenn ein Kamerastream mal abbricht. Also, wenn man, ich zeig euch, ich zeig euch
mal wie das geht. Also, wenn ich den Kamerastream starte, dann sag ich so was hier. Kamerastream-Pool
start und übergibt quasi ein Kameraobjekt, wo dann drinsteht. In so einem Kameraobjekt
steht folgendes drinne. Aber jetzt die Musik ein bisschen laut. Nee, passt so eigentlich,
oder? So, eigentlich ist es okay. Aber ein Ticken leiser. Ich glaub passt. So, in so
einem Kameraobjekt steht was folgendes drinne. Das ist schon committed. Ja, das hab ich vorhin
committed vor einer Stunde oder so. Das wir hier im Stream weitermachen können. Also,
jede Kamera hat ne ID, ne Name, ne Stream-Mode, User und Passwort, falls man das braucht.
Abneuer ich kein Bild, ja? Bei mir in Vorschau ist alles gut. So, dann startet er das, dann
sag ich ihm hier, starte, dann Stream für die Kamera. Es ist überhaupt nichts F, Leute.
Also, manchmal lass ich mich ja übelst debaten, ja? Hast du Git-Add-Punkt gemacht? Äh, weiß
ich nicht mehr, was ich gemacht hab. Ich glaub Git-Add-A. Kengin, was suchst du denn? Ah,
okay. So, also der startet hier ne Kamera. Was der dann macht ist folgendes. Okay, der
startet immer noch. Der geht hier rein in die Start-Methode. So, und diese Kamera-Stream-Klasse
oder diese Kamera-Stream-Pool ist dafür zuständig, quasi zu sorgen, dass die Kamera-Streams, wenn
sie abkacken, neu gestartet werden. Also, wenn ich hier sage, Kamera start, dann legt
er ein neues Kamera-Stream-Objekt an, addet das zu einer Streaming-Liste, das ist hier
so ein Threadsafe-Dictionary, wo er das reinschreibt. So, dann macht er einen neuen Task, wobei
ich mir jetzt gar nicht so sicher bin, ob ich den eigentlich brauche. Hm, mal gucken.
Ähm, da macht er einen neuen Task auf mit einer Endlosschleife drin, startet den Kamera-Stream,
wusstest du das get or add bei einem konkuren Ding? Nichts, Threadsafe hab ich letztens
auch gesehen, ja? So, der startet den Kamera-Stream. Wenn der Stream abkackt, dann lockt er ne Message,
resette das Ganze, wartet 5 Sekunden und fängt wieder von vorne an. So, und sollte das ordentlich
beendet werden, weil man die Kamera stoppt, dann geht er hier raus und ich muss neu starten.
So. Das hier unten ist die eigentliche Klasse, die dafür da ist, FFM-Pack, die Kamera-Streams
zu benutzen. Also, äh, die Kamera-Streams abzufragen, es funktioniert auch relativ
simpel, wenn man schon mal was mit FFM-Pack gemacht hat. Das hier ist alles bla bla, das
braucht kein Mensch. Wichtig ist das hier, das ist die eigentliche Funktion, die man
das macht. Ähm, setzt hier ein paar Options und hier, äh, Moment, und hier verbindet
sich zur Kamera. AV-Format, Open Input. Äh, also das Frontend kann ich dir zeigen, da
ist nicht allzu viel zu sehen. Test, Test-Kamera-Streams am Start, die übrigens grad nicht laufen,
wenn ich's gestoppt hab. So, äh, hier öffnet er den Kamera-Stream. Was echt verwirrend
ist, wo man bei FFM-Pack aufpassen muss, wenn das hier fehlschlägt, dann macht er automatisch
Free hier drauf. Was merkwürdig ist, weil überall anders musst du selbst Free machen,
aber nur da macht FFM-Pack Free für dich. Logik, weiß nicht. So, deswegen, deswegen
kann ich da auch nicht einfach weitermachen danach. So, da verbindet sich zur Kamera,
liest ein bisschen Zeug aus, sucht sich raus in den Videostream, schmeißt ne Fehler, wenn
es keinen Videostream gibt, weil ich kann da schlecht Video abfragen, ohne Videostream
von der Kamera zu bekommen. So, und dann hat er hier eigentlich nur ne lange Endlosschleife,
wo er nix anderes macht als, ähm, einfach Frame nach Frame von der Quelle einlesen.
Der macht quasi Read Frame, macht ein paar Sachen mit, nächster Durchgang, Read Frame,
Read Frame, Read Frame, und das macht er unendlich lange, oder zumindest so lange, bis einer
Cancel aufruft, macht er das. Das ist das, was er im Prinzip, was er in FFM-Pack macht,
einfach unendlich lange die Kamera lesen. So, und dann mach ich hier noch ein bisschen
Error-Checking drin, also ich guck zum Beispiel, falls die Kamera, das machen nämlich viele
billig, äh, China-Cams, die, wenn die Timestamps nicht stimmen, unter anderem hier gerade, wenn
die Timestamps nicht stimmen, dann überspringe ich einfach diesen Frame. Ist nicht so schön,
man könnte das auch umsortieren, das werd ich noch machen später, aber erstmal ist,
äh, skipp ich das einfach, wenn die Timestamps nicht stimmen. So, und dann guckt ihr, ob's
ein Keyframe ist, wirft das Event on Keyframe, und wenn's kein Keyframe, also bei jedem Frame
wirft er das On Next Frame Event, so. Das war im Prinzip alles, was er in FFM-Pack macht.
So, und der Verbindungsaufbau, das ist das einzige, was noch irgendwie ein bisschen komplexer
ist, bei der ganzen Geschichte, ist hier drin. Äh, wir müssen übrigens das Front, das Frontend,
alles ist da. Glaub mal, hier zu. Ähm, was der hier macht, ist folgendes, der holt sich
eine Session-ID, hier im Frontend. Äh, oh, ja. Connect ruft er auf, und Connect passiert
die eigentliche Sache, also sprich, äh, der, wo ist das Session-ID, der ruft sich eine
Session-ID ab, so, dann kriegt er hier eine Session-ID zurück, es wird eine neue Connection
schonmal angelegt, dann, das ist jetzt WebRTC-interne Sachen, das muss man sich jetzt nicht so genau
angucken, dann ruft er sich ab, was der Server für Bildformate unterstützt, und schickt
zurück, was der Browser für Bildformate unterstützt, und wenn die sich einig sind,
ist eine Session aufgebaut, und wenn eine Session aufgebaut ist, dann passiert folgendermaßen,
passiert das hier, wenn eine Session aufgebaut ist zwischen Browser und Server, dann geht
er zu diesem CameraStreamPool, ähm, und, äh, das ist quasi das Event, was aufgerufen
wird intern für jeden, für jeden Frame. Er sagt CameraStreamPool getOutput für diese
Kamera, und dann kriegt er, quasi bei jedem neuen Frame von der Kamera, kriegt er jetzt
hier einen Callback, und das schickt er an den Browser, in einem eigenen Task. Wohlgemänt
alles in einem eigenen Task, weil das soll asynchron funktionieren und nix hängen, falls
es mal irgendwie länger dauert. Das ist, also, man kann sich das durchaus angucken, denk
ich, das ist diesmal relativ ordentlich, was ich hier fabriziert hab. Was ganz cool ist,
ich hab ein paar neue Features von C-Sharp 8, ist das glaub ich, verwendet, und zwar,
was macht eigentlich mein Update, ist das durch? Ah, mein Update ist, exellent. Update
ist fertig, da kann ich das jetzt mal installieren. Und zwar, ich hab ein Feature von C-Sharp
8 verwendet, oder 7, 8, ich glaub von, ne, neun kommt jetzt von C-Sharp 8, wusste ich
selbst gar nicht, dass es das gibt, und zwar, await for each. Und ich bin mir da gar nicht
sicher, ob ich das in einem eigenen Task überhaupt machen muss, oder ob der das unter der Haube
für mich schon macht. Await for each, das ist eigentlich ziemlich, ziemlich praktisch,
und zwar, jedes Mal, wenn es von der Kamera ein neues Bild gibt, dann schreibt er das
hier in diese Queue rein, oder in diesen Buffer rein. Und, äh, hier passen maximal 5 Bilder
rein, und wenn mehr reinkommen als, also, wenn der Buffer voll läuft, dann droppt er
immer das älteste Bild. Weil, was soll er machen? Ja, da kann er den Buffer schlecht
voll laufen, da ist irgendwas der Ram, irgendwann ist der Ram dicht. Also, er hat maximal einen
Buffer von 5 Frames, der sich speichert. So, und was das hier macht, ist, der kann, also,
eigentlich ein richtig, ein richtig nicees Feature, der wartet quasi immer hier drauf,
wenn hier ein neues Bild reinkommt, dann führt er das hier, diesen Schleifendurchgang,
einmal aus, geht wieder nach oben, und wartet von sich aus, ohne dass ich da irgendwie einen
Sleep einbauen muss, oder irgendwas. Der nimmt sich das, bis hier der Buffer leer ist, bis
dieser Channel leer ist, und wenn der Channel leer ist, dann wartet er einfach, bis das
nächste Bild reinkommt. Also sprich, ich muss hier keine Endlosschleife machen, äh,
ein Sleep für zwei Sekunden, oder sowas, das macht das alles für mich. Das ist eigentlich
ein ziemlich nicees Feature, ich geh mal davon aus, dass man damit noch viel mehr nicee Sachen
machen kann. Also, es ist ziemlich pock, man muss Microsoft wirklich mal, wirklich loben,
ähm, die haben echt viel gute Sachen eingebaut in C-Sharp. Auch, auch die letzten Jahre
über. Also, C-Sharp hatte von Anfang an schon ziemlich gute Features, das liegt aber vor
allem daran, weil Anders Heilsberg übelst, äh, guter Sprachdesigner ist letztendlich,
jetzt ist er ja bei TypeScript am Start, glaub ich, weiß nicht, ob er da immer noch
ist. Ey, du kannst dich einfach MS loben, doch, was C-Sharp angeht, muss man wirklich,
oder auch.NET Core, muss man das loben. So, ähm, genau, so, jetzt machen wir mal ein
Benchmark, wie schnell das Ganze, wie schnell das Ganze ist. Wie gesagt, Hyperfine ist ein
Commando-Zeilen-Benchmark. Ups. Der führt das jetzt ein paar Mal aus und sagt mir, wie
lang das dauert. So, und man sieht, mein Rechner braucht aktuell ungefähr 100 Millisekunden
für einmal Bilderkennung. Das ist das gleiche Bild, was wir uns eben angeguckt haben. So,
wir können jetzt nochmal das andere, wir nehmen mal das bisschen größere Bild, da werdet
ihr sehen, dass es instant langsamer wird. Naja, so viel langsamer auch nicht. Weil jetzt
muss das noch resizen, vorher hab ich es resized. Also, mein Rechner braucht ungefähr 100 Millisekunden
für einmal Object Detection. Was natürlich nicht zu vergleichen ist mit dem Raspberry
Pi, der Raspberry Pi braucht wahrscheinlich eine Sekunde für ein Bild oder so. Werden
wir da mal sehen. Äh, aber das Coole ist auch dadurch, dass das Ganze im Docker-Container
läuft, kann ich ihm einfach sagen auf dem Raspberry Pi, ey, beschränk dich mal auf
maximal zwei CPU-Kerne. Und mein Ziel ist ja, dass das Ganze gut auf dem Raspberry Pi läuft,
deswegen mach ich ja da auch kein großes Encoding, Transcoding, Decoding oder so, sondern reiche
einfach nur die ganzen Bilder von der Kamera durch. Und das, also das läuft ultra performant
auf dem Raspberry Pi, das hab ich schon ausprobiert, aber halt nicht mit Object Detection. So,
und ich hab ja die Hoffnung, wenn man ihm sagt, ey, Object Detection einmal die Sekunde
oder einmal alle zwei Sekunden pro Keyframe, dass das ein Raspberry Pi auch gut ab kann
mit eins, zwei CPU-Kernen ausgelastet. Was natürlich richtig cool ist, weil dann hat
man eine richtig, ziemlich gute Erkennung von Kamerabildern, die viel besser ist als das,
was wir in den meisten China-Cams haben. So, und jetzt machen wir mal ein bisschen Big-Brain-Logik.
So, also ich weiß, dass das hier funktioniert, also wir haben uns angeschaut, Object Detection
funktioniert und das ist auch relativ schnell. So, und jetzt werden wir mal eine Object Detection-Klasse
programmieren. Gut, also, gehen wir da mal hin, hier. Im Zweifelsfall alles in Helper,
wenn man nicht weiß wohin, dann immer in Helper. Exzellent. So, und ich muss mir jetzt nebenbei
mal was aufmachen zum abgucken, weil alles aus dem Kopf so jetzt runterschreiben kann
ich auch nicht. So, was gucken. Weil ich habe schon FFmpeg und habe schon ein paar Projekte
verwendet. Ich gucke, ich mache mir da meistens auf dem zweiten Bildschirm eins auf, dass
man ein bisschen abgucken kann, weil FFmpeg ist in der Handhabung ein bisschen G-Bait-lastig.
Und ich zeige euch jetzt mal was, was ein mega G-Bait bei FFmpeg ist. Wo ich mal echt
fast einen Tag lang danach gesucht habe. So, und zwar es gibt FFmpeg, Free Packet, ja
mittlerweile ist es Obsolete Deprecated, ja ja, mittlerweile ist es das. Es gibt FFmpeg,
Free Packet und es gibt FFmpeg, Packet Free. Und das ist mega scheiße, das ist ultra scheiße.
Und die machen auch noch im Prinzip das gleiche. Der eine nimmt einen Pointer zu einem Pointer.
Übrigens, Chat, was mein, ok ich habe es eben schon gespoilert, aber was meint ihr,
welches davon das richtige ist? Man hat ja so, wenn man sich so anguckt, eigentlich gar
keine Ahnung, oder? Chat, was meint ihr, was ist die richtige Funktion, die man eigentlich
verwenden will? Free Packet oder Packet Free? Ich habe es eben schon gezeigt, was Deprecated
ist, aber trotzdem. Ja, das zweite ist richtig. Das ist das, was man nehmen soll, das soll
man nicht mehr nehmen. Und als Unwrap, also Fail, da habe ich mal wirklich einen Tag danach
gesucht, weil es irgendwie überhaupt nicht funktioniert hat. Deswegen brauche ich immer
was zum abgucken, denn FFmpeg ist schon ein bisschen Chebatelastig an manchen Stellen.
So, also was ich mir überlegt habe, ist folgendes. Ich habe euch jetzt einmal so grob gezeigt,
was das intern macht, und ich denke mal, man kann das halbwegs nachvollziehen. Wenn nicht,
wie gesagt, der Sourcecode ist ja hier am Start. Oh, Exquisite-Sternchen. Kann man sich auch
mal angucken. Also ich finde, es ist relativ ordentlich programmiert. Es hat zwar kaum
Comments, aber ihr wisst ja, wer braucht Kommentare, wenn er ordentlich den Sourcecode schreibt.
Kommentar ist eh total überbewertet. Ist der Namespace echt? Wo habe ich mich vertippt?
Das ist Absicht. Natürlich ist Monarch-S. Geht mal Monarch-S ein im Chat. Das ist Absicht.
Conker-S ist ja der, und Monarch-S ist der. Das ist Absicht. Eigentlich eine ziemlich
bescheuerte Name für ein Projekt, weil niemand wird, wenn es um Webcams oder Streams geht,
nach Monarch-S suchen, Alter. So, also was ich mir überlegt habe, ist folgendes. Wir
können jetzt hier irgendwie so keine New Object Detector oder sowas machen. Ich kann nicht
mal schreiben. Also wir machen irgendwie New Object Detector. Ich überlege jetzt nur mal,
wie wir es machen. Also wir bauen eine Klasse, die für uns, also die nicht wirklich Object
Erkennung macht, sondern die diesen Object Detection Service verwendet. So, das Ding
schmeißt dann irgendwie ein Event. Das Ding schmeißt dann ein Event. Irgendwie sowas
wie Detected. Und da subscribe ich mich dann dran und schicke es zurück zum Client. Soweit
die Idee. Ergebnisse. Kann ich schreiben. Ergebnisse. Doch, passt. Aber ihr wisst was
ich meine. So habe ich mir das gedacht. Also, probieren wir das mal aus. So, ein bisschen
FFmpeg Zeug muss ich mir abgucken. Weil aus dem Kopf, wie gesagt, klappt das nicht. Also,
ich will sowas hier in der Richtung machen. Also auf jeden Fall wird das was, was Ressourcen
aufräumen muss. Event, Task, Assync, Await. Was? Choose my name. What? Das sind jetzt
so paar Stichwörter, die sich den Zusammenhang bei mir im Gehirn gerade nicht ergibt, was
du wissen willst. Hand auf Tief. Moin. Also, ich brauche hier sowas wie War. Nennen wir
es mal irgendwie Object Detector. Brauchen da noch ein ordentlicher Namen für. Object
Detector. So, irgendwie sowas. So. Und das wird auf jeden Fall, muss das seine Ressourcen
wieder aufräumen, weil da wird FFmpeg drin gemacht. FFmpeg ist unsafe, Pointergeschiss
und das geht auf jeden Fall in die Hose, ohne dass ich das aufräume am Ende. Wir brauchen
natürlich auch die Object Detector Manager Factory, sonst geht es natürlich nicht.
Das ist klar, wir müssen jetzt ja wie machen, wie die Großen in Java müssen wir das machen.
Okay, also wir implementieren jetzt erstmal diese Object Detector Class. Das machen wir
hier drin. Normalerweise sagt man immer ein Pfeil pro Klasse, aber mal draufgeschissen.
So, Class, Object Detector, Public. Was machst du da? Ist das hier weg? Public, da, Object
Detector. So, also was will Choose My Name wissen? Event oder, nee, kannst du das ja
entweder mit Events machen oder Task Objekte erzeugen und evaden. Das ist ein bisschen
unterschiedlich, würde ich sagen. Also Events, also man kann natürlich letztendlich alles
für alles verwenden, aber ich würde sagen, es gibt nur zwei grundsätzlich unterschiedliche,
nennen wir es mal, Sorten von Events. Also es gibt einmal die klassische Eventgeschichte
in.NET. Das ist ja quasi ein Push. Also du subscribes dich und zu dir pusht dann einer
irgendwie das Event. So, und dann gibt es quasi noch die umgedrehte Sache. Du kannst
das Ganze pullen. Also quasi es hängt, bis es was gibt und du pullst quasi das Event
raus aus dem Objekt. Also quasi das hier, äh, das hier ist quasi ein Pull. Ah, der
liest quasi, der zieht sich aus dem Channel quasi die Events raus. Und das hier unten
ist quasi ein Push oder das hier ist ein Push Event, weil jemand, der sich dahin subscribt
hat, er kriegt ja das Event zu sich hingeschoben. So, und deswegen würde ich sagen, es ist
ziemlich situationsabhängig, was man nimmt. Das schöne an Events ist, dass du mehrere
Sachen damit abfackeln kannst. Also du kannst, wenn du ein Event wirfst, können das beliebig
viele Leute empfangen. Aber, und da muss man aufpassen, der Eventhändler läuft immer
im Thread des Objekts oder dessen, der das Event auslöst. Also sprich, es hat überhaupt
keinen Sinn, wenn ich hier unten dieses Event werfe, Next Frame, und dann in einem Eventhändler
was mache, was zwei Sekunden dauert. Das, was passieren wird, ist, das plockt zwei Sekunden
an der Stelle, weil der Eventhändler immer im Thread vom Aufrufer läuft. Ich hoffe,
man kann mir irgendwie halbwegs folgen, was ich da erzähle. Das heißt, du musst dann
in dem Eventhändler einen neuen Task machen, sonst plockst du das. Also Events sind absolut
Null Multithreaded. Wo bin ich jetzt übrigens gerade stehen geblieben? Ok, Object Detector.
Task Run Event Evoke, das kannst du auch machen, dann hast du aber das nächste Problem. Du
kannst dir nicht mehr sicher sein, dass die Events in der richtigen Reihenfolge kommen.
Multithreading ist ganz schön scheißgebiet, es ist wirklich sacked, mega. Du musst an
so viele Sachen denken. Wenn du Task Run Event Evoke machst, dann kannst du dir nicht mehr
sicher sein, dass es in der richtigen Reihenfolge passiert. Also dass sie das in der richtigen
Reihenfolge empfangen. Das ist, also Multithreading ist richtig kacke an vielen Stellen. So,
Public Glass Object Detector. Äh, warum? Warum äh? WTF? Hä? Hallo? Achso, ich bin
ja kacke du. Ich habe ja gesagt, dass es disposable ist. Das ist auch ein wirklich
Big, jetzt will Ryder zu clever sein. Das ist auch ein ziemlich Big Brain C Sharp Feature
und zwar, das ist jetzt auch neu seit C Sharp 7 oder so. Man kann jetzt, wenn man Variablen
anlegt, man kann Variablen nicht nur so anlegen, sondern man kann jetzt noch Using davor
schreiben. Was das macht ist, sobald das hier außer aus dem Scope geht und das Scope ist
das hier, Klammer auf Klammer zu. Also sprich, sobald das Objekt hier aus dem Scope raus
geht, wird automatisch die Aufräum Methode aufgerufen. Das ist eigentlich ziemlich cooles
Feature, was die jetzt eingebaut haben. Das Feature an sich gibt es schon ewig, aber
dass man das so schreiben kann. Früher musste man das so schreiben. Früher musste man das
so schreiben und dann hatte man übelst viel Eindrückungstiefe und so und man musste,
wenn man da schachteln will, wenn man da schachteln will, musste man das dann so machen. Das war
mega hängen geblieben. Also man kann in C Sharp die Sachen auf zwei Arten aufräumen,
wenn man Müll verursacht im Speicher. Man kann das auf zwei Arten aufräumen. Allerdings
man braucht beide Arten, wenn man es richtig machen will. Es gibt einmal diese Sache mit
Using und Disposable, also sprich das hier. Das heißt, wenn dieses Objekt hier aus dem
Scope geht, wird das hier aufgerufen. Hier mache ich dann hier Cleanup. Das ist quasi
die manuelle Variante. Das ist gut für unmanaged Ressourcen. Also wenn ich hier FFmpeg-Pointergeschiss
anlege, ist das hier eigentlich gut. Und dann gibt es noch die Destructor-Variante. Das
ist quasi die Garbage-Collected-Variante. Das hier, sobald alle Referenzen auf dieses
Objekt weg sind, ruft C Sharp irgendwann und die Betonung ist auf irgendwann, weil das
ist nicht garantiert wann, ruft der Garbage-Collector irgendwann meinen Destructor auf. Da kann
ich auch drin aufräumen. Aber ehrlich gesagt, wenn man unmanaged Ressourcen hat und im Zweifelsfall
auch schnell unmanaged Ressourcen anlegt, ist die Variante schöner, weil hier kannst
du dir sicher sein, dass die Ressourcen hier freigegeben werden, direkt nachdem du sie
nicht mehr brauchst. Und je nachdem, was du unmanaged machst, willst du, dass muss das
vielleicht sogar so sein. Du kannst nicht darauf warten, dass irgendwie zehn Sekunden später
hier mal ein Destructor aufgerufen wird. Also das hier ist die Managed-Variante, wo
C Sharp quasi das komplett für dich selbst, für dich handelt. C Sharp guckt, okay, ist
dieses Objekt irgendwie noch erreichbar, wenn nein, Destructor aufrufen. Dispose ist quasi
so die manuelle Variante für unmanaged Ressourcen, die man selbst aufrufen muss. Die andere Variante
wäre übrigens das hier. Das wäre die andere Variante und so ganz stimmt das auch nicht,
denn in Wirklichkeit ist es nicht das, sondern, also was dieses Using macht, ist folgendes,
das macht nicht nur einfach die Dispose, da macht er auch einen Try-Catch-Final-Lied
drum, sodass das immer aufgerufen wird. Also du kannst dich darauf verlassen, wenn du dein
Objekt mit Using anlegst davor und du hast in deinem Objekt Dispose-Methode, du kannst
dich darauf verlassen, selbst wenn das crasht, also selbst wenn das hier crasht, dann wird
immer noch der Clean-Up-Code aufgerufen. Also sprich, das ist wirklich Big Brain durchdacht,
was die C Sharp Leute da gemacht haben. Also ist schon wirklich nice. Deswegen kotz ich
auch immer rum, wenn ich was in C++ machen will, weil in C Sharp ist vieles so leicht
und in sich durchdacht. Und das heißt halt in C++ absolut gar nicht. C++ ist einfach
ein weherer Haufen von Leuten machen wehre Sachen und schreiben einen noch wehren Standard
drum herum und dann kommen dann irgendwelche C++ Sachen bei rum. So, also ich muss jetzt
mal ein bisschen was machen, wir haben ja noch gar nichts. Wir haben ja noch wirklich nichts
gemacht. Also, okay, also das war jetzt ziemlich klasse. Disposable haben wir. So, also, lasst
mal überlegen, was, was brauchen wir? Was brauchen wir? Also wir haben, wir haben Dispose,
wir brauchen eine Public, irgendwas Public, Public Detect. So, wir brauchen eine Methode
für, die wir aufrufen für Object Erkennung. So, das machen wir hier. Object Detector,
Object Detector Detect. Und der kriegt, ach Moment, ja fast, fast. Kamerastream on Keyframe,
also sprich immer wenn es einen neuen Keyframe gibt, dann soll er sich subscriben zu dem
Event und soll das hier aufrufen. So, also was das hier macht ist, jedes Mal, wenn es
ein Keyframe von der Kamera gibt, dann ruft er das hier auf. Warum keine Cancellation
Talks? Hat keinen tieferen Sinn. Also jetzt nicht so, dass ich gesagt habe, nee an der
Stelle keine Cancellation Talks. Ähm, an der Stelle habe ich es hier, hier habe ich
es einmal manuell gemacht. Ah, hier, Reset, Break Cancel, genau. Hier mache ich es mit
einer Variable, die die Schleife am Ende abbricht. Ja, man könnte das auch alles mit Cancellation
Talks machen, das ist richtig. Ich meine, wir können hier auch Cancellation Talks übergeben,
aber ich gehe mal davon aus, es hängt nicht so lang und dann hängt es halt mal eine Sekunde
oder so. Das probieren wir mal aus. Müssen wir es mal rantasten. Ich weiß ja noch gar
nicht, wie das am Ende aussieht. Also das hier wird aufgerufen. Was hat er übrigens
für Schmerzen mit Static? Nein, bestimmt nicht. So, also hier kommt jetzt das Ganze
rein und hier bekommt er das Bild, hier bekommt er quasi das encodete, also noch das komprimierte
Bild von der Kamera. Den komprimierten Keyframe von der Kamera kriegt er hier jetzt. So, ähm,
so, was wir jetzt machen ist, wir machen das jetzt ähnlich wie hier, wo ich das alles
in so einen Channel reinschreibe. Wir machen uns einen Buffer. Einen Buffer für Kamerabilder.
Also, keine Ahnung, was machen wir? Private Channel für ein Kamerabild. Also dieses AVPacket
Handle ist quasi ein Bild von der Kamera, steckt da drinne. Guckt, ich zeig's euch mal.
Es gibt das Byte Array, das sind wirklich das komprimierte Kamerabild, wie lang das
Segment ist und Höhe, Breite. Das ist was, was ich drumherum gebastelt habe, dass der
Speicher automatisch aufgeräumt wird. Weil ansonsten müsste ich überall dieses Packet
unref machen, freeen und sonst was und das macht er hier für mich, deswegen hab ich
das mal so gerappt. So, also, wir machen einen Buffer. Oder nennen es wir irgendwie, images
oder so. Images. Gleich. Create. Und was machen wir da rein? Wie viel passen da auch rein?
5? Nee, eigentlich einer. Einer reicht. Ist genug. Weil danach ist es eh zu spät. Juckt
eigentlich nicht. Maximal einer. Reicht. Weil was interessiert mich, ob ich motion
... Also mal angenommen, ich produziere so viele Bilder, dass meine Motion Detection
beziehungsweise Object Detection nicht hinterherkommt. Dann hat's auch gar keinen Sinn, wenn ich
da noch ältere Bilder drin hab, weil dann seh ich dann ne Motionerkennung von vor 5
Sekunden, die keinen mehr interessiert. Da ist ja der Typ schon längst durchs Bild gelaufen.
Also entweder das aktuelle Bild oder gar kein Bild. So. Also, das heißt, das einzige,
was wir hier in dieser Detect Methode machen, ist Images, äh, was nicht? Triad? Ah nee,
Writer, Triad. Nee, Tri-Write ist es. Tri-Write. Dann ist Packet. So. Also sprich, jedes neue
Bild von der Kamera... Hallo? Hotel? Wo ist es? Ja. Jedes neue Bild von der Kamera wird
hier reingesteckt. Copper Pride. Dann wird's in diese Warteschlange geschrieben, die ich
dann in der Schleife auslese. So. Was wir jetzt noch machen müssen, ist, das Bild von
der Kamera ist ja komprimiert. Äh, Geed95 ungefähr. Es macht noch ein bisschen mehr.
Guck, ich kann's dir zeigen hier. AVPacketHandle macht folgendes. Ähm, setzt am Anfang erstmal
hier diese ganzen Felder. Breite, Höhe. Äh, einmal kopiert das das Ganze in ein Managed
Array. Byte Array für die ganzen Daten. Setzt die Duration. Und ansonsten, mehr macht das
nicht. Wenn's out of Scope geht, wird das Ganze freed. Also, das war's. Mehr macht's
nicht. Ja, und falls ich mal ein bisschen ekliche Sachen machen will, dann auch das hier. Ja,
und ich hab noch was gebastelt, dass man, ähm... Empty? Wo hab ich's? Äh, dass man
das hier folgendermaßen aufrufen kann auf AVPacketHandle. Empty. Falls man mal ein leeres
Package hat und man möchte keinen Nullpointer verwenden auf irgendwas, was es nicht gibt,
dann kann man das verwenden. Weil, es kann ja mal sein, dass man eine Methode hat, die
den Package zurückgibt und das hat dann einen Fehler und man möchte jetzt keine Exception
schmeißen, sondern einfach ein leeres Bild zurückgeben oder so. Ja. So, also wir schreiben
das jetzt hier rein. Gut. Jetzt ist ja das Bild immer noch komprimiert. Was heißt, wir
müssen jetzt sowas hier machen wie private, uns, das ist auf jeden Fall unsave, ähm... Und
das gibt jetzt tatsächlich ein neues Package zurück und das decoded. Decode. Decode and
resize. Wir müssen ja das Bild, was wir kriegen von der Kamera, also das ist ja ein komprimiertes
H.264 Bild aus einem komprimierten H.264 Stream. Das müssen wir quasi dekomprimieren
oder decoden und wenn wir es decoded haben, dann noch resizen. Das ist ja in dem Fall
ist es ein 1.920 x 1.080 komprimiertes Bild und das müssen wir decoden und anschließend
verkleinern und dann noch als, keine Ahnung, als JPEG encoden oder so, also irgendwas encoden,
was dieser, was dieser Object Detection-Dings frisst. Was frisst denn das? Was frisst das
denn für Bilder? JPEG? Ja, also. It can read Bitmaps, PNG, JPEG und PPM. Was ist denn,
was PPM habe ich? Das ist irgendein Bitmap Format, PPM. Ja, wir machen es, wir encoden
es als JPEG. Als JPEG. Also wir brauchen sowas, decode and resize und jetzt müssen wir FFmpeg
Magic machen und das copy und paste ich mir ein bisschen rüber aus einem anderen Projekt.
So, jetzt muss ich mal kurz, jetzt muss ich mal kurz in mich gehen. So, also das hier
kann ich mir schon mal rübercopy pasten. Memory Leaks incoming. Oh ja, wahrscheinlich
werden wir irgendwelche Memory Leaks produzieren. Garantiert. So. Ah, ich muss die Decoder erst
noch anlegen, weil ich habe ja gesagt, das ist ja noch encoded, das heißt, ich muss
es erst noch decoden. Hast du eine Empfehlung für einen bezahlbaren 10 Gigabit Switch? Ja,
Microtech. Beste. Okay, dann machen wir jetzt mal Big, Big, Big Brain Logik und zwar wir
nehmen mal sowas hier. Private bool initialized. Hab ich das jetzt richtig geschrieben? Initialized.
Hey, ich hab zwar erst mal richtig geschrieben. Here, false. Dann gucken wir hier in der Object
Detection drinnen. Ob das schon. Nee, das waren wir hier. Genau, hier gucken wir. If,
wenn das noch nicht initialized ist, dann machen wir Init und dann machen wir initialized is
true. Der Sinn ist dahinter, weil, ihr werdet es jetzt gleich sehen, um FFmpeg Sachen anlegen
zu können. Ich kopiere mir das jetzt gerade mal raus. Um in FFmpeg Decoder und sowas angeben
zu können, braucht man die Größe von dem Bild. Und die Größe von dem Bild habe ich
ja beim ersten Aufruf von hier erst. Das heißt, beim ersten Aufruf mit einem Package
werde ich die ganzen Decoder anlegen. Und damit ich das nicht jedes Mal mache, habe
ich hier dieses Flag, was dafür sorgt, dass ich nicht crashe, weil wenn ich die oder Memory
leake, wenn ich jedes Mal die Decoder und Encoder neu anlegen würde. So, also sagen
wir hier Private noch mal unsave. Nennen wir irgendwie sowas. Init, Decoder, Encoder. Das
braucht auch das Packet. So, und jetzt kommt Big Brain FFmpeg Matchup. Was habe ich verkehrt
gemacht? Achso, unsave, void. 1, 3, 3, 7. Ja, ja. Da hast du recht. Ist besser, wenn man
es hier reinschreibt. Was genau heißt bzw. 8 unsave, wenn du Libraries verwendest, die
C-Funktionen aufrufen und dort irgendwelche Pointer hin und zurück geschickt werden,
so wie zum Beispiel hier bei FFmpeg, dann erlaubt das die Sharp normalerweise nicht.
Dann musst du sagen, das ist unsave. Weil alles, was irgendwie Raw-Pointer und so verwendet,
ist unsave. Heißt, jedes Mal, wenn du mit irgendwelchen C-Libraries interagierst, wirst
du früher oder später unsave verwenden. Unsave kann man auf drei verschiedene Arten verwenden.
Du kannst wieder an der Methode dran, an der kompletten Klasse dran oder als unsave block
innerhalb einer safe Methode. Ach ja, und by the way, eine ganz wichtige Geschichte.
Wenn man unsave verwendet, kann man kein async await verwenden in der gleichen Methode. Also
sobald ich hier sage, das ist unsave, ich kann nicht unsave async sagen und dann irgendwie
sagen await task completed. Das geht nicht. Sobald ich das versuche, sagt er mir, hier
guck mal, cannot await in unsave-Kontext. Das ist ASP.NET Core richtig, ja. So, ich muss
jetzt mal ein bisschen zu Potter kommen, das wird ja heute mal wieder gar nichts. Ich habe
drei Zeilen, drei Zeilen geschrieben bisher. Okay, unsave. Also, wir initialisieren jetzt
unsere Decoders. Okay, dann, ich copy-paste mir mal rüber. Zack, copy-paste, das da. Bäm.
Das ist FFmpeg Magic, ich suche mir Decoder für H.264. 99% der Cameras streams in H.264,
das ist das einzige, was wir supporten. Und Encoder, das ist der Encoder für JPEG. Als
Frontend, ne, das Frontend ist noch relativ übersichtlich. Also, das Frontend besteht
im Prinzip nur aus ein paar Videoplayer für die Kamerabilder. Das ist zwar jetzt mit
Vue.js gemacht, aber das ist noch so simpel, das kannst du mit jedem x-beliebigen anderen
auch machen. Also, ich benutze Vue.js hier als Frontend-Framework und SignalR als Messaging,
wie nennt man das? Messaging-System. Weil, zum Beispiel, wenn man was erkannt hat, das
werden wir auch gleich sehen. Also, im Angenommen, ich hab jetzt ein Objekt erkannt auf einem
Bild, dann muss ich halt dem Frontend Bescheid sagen, hallo, ich hab ein Bild erkannt, dass
das Frontend dann quasi so einen Viereck malen kann hier drauf, wo er was erkennt hat.
Das machen wir jetzt alles. Also, Decoder-Encoder, also wir suchen H.264-Encoder raus. So, dann
brauchen wir einen, hab ich wieder BigBrain benannt, einen Resizer. Also, der Resizer
macht das, was man sich anhand des Namens denkt, er resized. Wer hätte sich das nur
denken können? Das ist auch FFmpeg-Magic, das müssen wir uns jetzt gar nicht so sehr
im Detail angucken. Der nimmt sich ein Paket und resized das auf kleinere Größe. Zumindest
brauchen wir das, weil, ihr habt's ja gesehen, das hier frisst am liebsten Bilder in der
Größe 300x300. Und 300x300 ist halt so ein bisschen kleiner als 1920x1080. Das heißt,
ich muss das resizen. Also, sagen wir jetzt hier irgendwie New, Frame, Resizer. So, also,
was brauchen wir? Source ist Paket, ah, Moment, Moment, Moment, Size. Genau, ist Paket Breite
und Paket Höhe. Dann brauchen wir Source Destination. Source Destination ist einfach, 300x300. Wir
wollen das resizen auf 300x300, weil diese Object Detection, diese Object Detection API
am liebsten 300x300 große Bilder frisst. Dann brauchen wir, was kommt als nächstes? Pixel
Format. Das Pixel Format ist übelst kryptisch, wenn man sich noch nicht mit Videos beschäftigt
hat. Natürlich Hardcoded. Das ist ja Hardcoded in dem Object Detection drin. Die wollen 300x300
Bilder haben. Die Input, der Input ist dynamisch, je nachdem wie groß das Package ist. So,
Pixel Format. Das Pixel Format Hardcoded wir, weil wir unterstützen nur H264 Webcam Streams
und die sind immer H264 und quasi immer dieses Pixel, also dieses Farbpixel Format. Das hat
übrigens auch den großen Vorteil, dass wir das direkt zu einem JPEG umwandeln können,
weil JPEG intern das gleiche Pixel Format verwendet. Da müssen wir gar nichts umwursten.
So, Keep Aspect Ratio ist false tatsächlich. Wir wollen das strecken und dehnen und machen
und tun. Also was das macht, ist mal angenommen, ich hab nen 1920x1080 Kamerabild und ich will
das resizing zu 300x300. Das ist ja das falsche Seitenverhältnis. Das eine ist 16 zu 9 und
das hier ist 1 zu 1. Das heißt, was man jetzt machen kann, ist entweder beim resizing das
jetzt ja entweder abzuschneiden oder aber es eben nicht 300x300 zu machen, sondern frag
mich nicht, 300x120 oder irgendwie sowas. Dass das Seitenverhältnis erhalten bleibt
oder man streckt es und das mach ich hier, also sprich das Seitenverhältnis wird dann
anders, aber dieser Object Detector, der möchte die Bilder immer in 300x300 haben, dann funktioniert
er am besten und dann kann ich es auch selbst resizing. Bringst hier das ein cooles Feature
aus Rider, wenn ich jetzt sag hier, ich will diese Resizer Variable belegen und die gibt
es nicht, deswegen ich mag Rider richtig gern mittlerweile, kann ich sagen, okay hier gibt
es nicht, Create Field Resizer, da legt er mir automatisch hier oben was an für die ganze
Klasse, die Variable, wo ich das dann reinschreiben kann, da muss ich nicht extra von der Hand
hingehen und das reinschreiben. Eigentlich ziemlich nice. So, Resizer. So, dann brauchen
wir, das kann ich jetzt wieder copypasten aus dem anderen. Im Prinzip kann ich mir eigentlich
alles, fast alles copypasten, also wir legen uns einen Codec an für H.264-Decoder, wir
legen uns einen, den nennen wir übrigens anders, einen JPEG-Encoder an, nicht ENC, wenn man
es richtig macht, dann passt das auch. Also, einen Encoder für JPEG legen wir uns an, dann
müssen wir ein paar Settings einstellen, irgendwie eine Bitrate vergeben, ups, fuck, warum hat
das jetzt nicht kopiert, eine Bitrate vergeben, achso, da heißt es jetzt natürlich anders,
dass das Bild nicht total verpixelt ist. So, dann brauchen wir Encoder, die Breite, die
Breite ist nicht die Breite vom Package, weil das ist ja die Quelle, hier ist ja quasi
jetzt nach dem Resize, das heißt wir müssen sagen Resizer, Destination, Breite und Höhe,
also das generiert quasi ein kleines Vorschaubildchen, 300 mal 300, das nimmt, das nimmt das Bild
von der Kamera, decodet es, verkleinert es und komprimiert es als JPEG, oder eigentlich
muss es das gar nicht großartig komprimieren, weil das Farbformat schon stimmt, der kann
es einfach speichern wieder als JPEG, oder ja ein bisschen, doch der komprimiert es schon,
aber ihr wisst was ich mein. Wir könnten auch, wir könnten richtig, ey kann das Ding
WebP oder sowas, WebP, dann nehmen wir WebP, alter, nee, WebP kann es nicht, WebP ist ja
so der neueste Schrei gerade. So, das ist Höhe, hat jemand von euch schon mal WebP oder
Dings, wie heißt die, wie heißt diese Alternative zu WebP, wow, wie ist das, ah ja, A, A, A,
A5 oder so, A5, heißt das, nee WebM ist quasi WebP, nur für Videos, also WebM und WebP ist
das gleiche, genau A5 meine ich hier, das da, Hive, was zum Teufel ist Hive, hier gucken
wir mal, Browser, Image Format Support, Mozilla, was sagt Mozilla dazu, APNG, A5 meine ich
hier, AV1, Image Format, genau, ja, das kann FFmpeg noch nicht, sonst würde ich das nehmen,
naja, gut, machen wir weiter, dann muss ich das hier, die Sachen kann ich auch kopieren,
sehr gut, exzellent, heißt jetzt hier nur anders, so und dann müssen wir das ganze starten,
so was hier jetzt passiert ist, okay, Hive ist das Apple Format, irgendwas habe ich hier
gerade verkackt, was macht der File, das ist das gleiche wie ein C++, das ist Pointer,
Pointer dereference, also du machst nicht Punkt, sondern du machst Pointer, weil das
hier ist, das hier ist unsafe, das ist ein Zeiger auf einen AV-Codec-Kontext und der
File dereferenziert das und greift dann darauf zu, das ist das gleiche wie ein C++, normalerweise
sieht man das in C sharp nicht, weil man in C sharp eigentlich kein unsafe Zeugs macht,
es sei denn, man interfacet hier mit FFmpeg, übrigens hier, was habe ich hier vergessen,
unsafe, mag er nämlich nicht, so, gut, also wunderbar, das funktioniert doch schon mal
ganz gut, so jetzt habe ich hier meine Decoder angelegt, so und jetzt kommt das nächste,
wir testen mal, was hat er hier für Schmerzen, warum mag er nicht, wo ist der, achso, so
jetzt gucken wir mal, ob das ganze funktioniert und nicht gleich crasht, Stream-Uhr, ja komm
wir legen, wir nehmen nur mein iPhone mal rein, dass das ganze schneller startet, Tobonautilus
drei Monate, Dankeschön für den Sub, exellent, exellent, Subscription, so wir legen hier
nur mal die Kamera an von meinem iPhone gerade und dann vertraten wir doch das ganze mal,
dass das jetzt auch aufgerufen wird hier, also, also habe ich schon, On Keyframe, Object Detector
Detect, alles klar, probieren wir mal aus, ob das funktioniert, Detect, gutes, altes
Print F, Debugging, ok, ja, das funktioniert, sehr schön, gucken wir mal in Browser, da
ist das Bild von meiner Kamera, ok, sehr gut, man sieht ja auch unten, es läuft, ne, also
es, wow, es funktioniert, die, es ist tatsächlich Live-Kamera-Bild und danach nehmen wir gleich
das von meinem Hoftor, ich muss das, ich muss die Warnung mit dem falschen Zeit-Stempel
wegmachen, das geht mir auf den Keks, kriege ich Anfälle, wenn das permanent hier durch,
durchflimmert, wo ist das hier, Wrong TS, ja, schön, schön für den Timestamp, wenn
er falsch ist, oh, jetzt habe ich es zugemacht, ich kann't noob, exellent, so, also, der ruft,
bei jedem Keyframe ruft er jetzt meine Object Detection Methode auf, schon mal gut, so,
und jetzt müssen wir das irgendwie in den Decoder füttern, also, jetzt lass mal überlegen,
das heißt, hier machen wir nix, so, Decode and Resize, wenn nicht Init, dann rufen wir
auf, Init Encoder, alles klar, gut, dann ist das Ganze initialisiert, sehr schön, das lassen
wir mal, und jetzt müssen wir im Hintergrund, jetzt brauchen wir einen Hintergrund-Task,
der permanent guckt, ob es neue Bilder von der Kamera zum analysieren gibt, also, ich
schreibe das jetzt mal so runter, mal gucken, ob das Sinn ergibt, was ich da mache, so,
also, Private, wir nennen es mal ganz big brain, Name Loop, ich nenne immer meine Sachen
immer Loop, manche nennen es Run, manche nennen es Do It, manche nennen es Do Work oder Work
oder was auch immer, also sprich, hier ruf ich mal, ich dann meinen, meinen, meinen Task
drinne, der im Hintergrund überprüft, was Sache ist, so, das machen wir auch schon mal
Async, weil ich mal ziemlich sicher bin, dass ich das gleich brauche, so, das starten wir
hier drin, dann sagen wir Newt, wobei machen wir, brauchen wir überhaupt einen neuen Task,
das ist eine gute Frage, ich hab ehrlich gesagt keine Ahnung, ob wir einen neuen Task brauchen,
wir nehmen auf jeden Fall schon mal ein neues C-Sharp-Feature, wir nehmen nicht, nehme
ich wieder Async oder Await For Each und zwar nehmen wir hier meine, die Image, die Collection
mit dem, mit dem, oh, also sprich, der liest jetzt unendlich lange neue Bilder ein, die
eben hier reingeschoben, hört sich jetzt ein bisschen sehr an, aber die ihr hier reinbekommt
in diese Message oder in diese Queue und die liest ihr unendlich lang ein und guckt ob
es da was neues gibt, okay, also, ich bin mir jetzt nicht sicher, muss ich das jetzt
in einem eigenen Task machen oder nicht, muss ich das im eigenen Task machen, ich hab ehrlich
gesagt keine Ahnung, mach das für mich jetzt, zählt das schon in einem eigenen Task oder
nicht?
Und in welchem Task oder in welchem Thread-Context läuft das jetzt, wenn ich das jetzt einfach
so mache?
Ich hab keinen Plastenschimmer, na gut, ja, wir machen mal weiter, wir machen mal weiter,
so, also, ich lese unendlich, ich muss mal kurz gucken, ob der überhaupt, wir machen
jetzt hier exquisite printf-debugging wieder, wir gucken mal, ob der das überhaupt ausführt
jetzt, eins, zwei, okay, führte aus, alles klar, funktioniert, so, also, For Each, so,
hier drin machen wir jetzt unsere Object-Erkennung und wir brauchen dann noch ein Event, würde
ich sagen, wir brauchen noch ein Event, Public, Event, Action, das nennen wir dann irgendwie
sowas wie Motion, nee, nicht Motion, nicht Motion, Object, Object Detector, Result oder
so, das müssen wir noch anlegen, das ist alles nur mal Platzhalter, weil ich noch nicht
genau weiß, wie der ganze Mist am Ende heißen wird, so, das ist ein Event, das schicken
wir raus, wenn wir was erkannt haben auf dem Bild und die Ergebnisse drinnen, Chat sagt
gerade, hatte mal einen Entwickler bei uns, der Methoden öfters mal Let's Do It genannt
hat und Methoden gehabt, wo er bool umgedreht hatte von if zu, von true, return, false,
oh, das ist Big Brain, Logik, ja, hat er das gemacht, um euch zu ärgern oder konnte er
das nicht anders, so, das nennen wir jetzt hier Detected, gut, das heißt, was wir jetzt
jetzt machen ist, wenn, ich laber das jetzt nur so vor mich hin, weil ich nachdenken muss,
also, wenn ich ein neues Objekt erkannt habe, dann schick ich das zum Client, so, ich glaub,
das ergibt halbwegs Sinn, so, und jetzt muss ich die eigentliche, die eigentliche Objekt
Erkennung machen, das wird jetzt ein bisschen komplizierter, also, ich mach die halt zum
Glück nicht selbst, ich schick die an diese API, aber, so, also, await for each, sprich,
ich gucke mal kurz, ob das auch funktioniert, blub, da sollte jetzt ganz oft blub ausgeben,
pro Keyframe sollte jetzt blub ausgeben, wenn das richtig funktioniert, blub, blub, blub,
blub, ok, exellent, funktioniert, so, und hier muss ich jetzt den API Aufruf drin machen,
also, wir brauchen auf jeden Fall schon mal einen HTTP Client, HTTP Client, um Webrequest
zu machen, so, dann brauchen wir eine Variable für das Image, und das ist, this is the code
and resize, genau, wir müssen, wir wollen es decoden und resizen, was ich übrigens noch
gar nicht implementiert hab, fällt mir gerade auf, das muss ich machen, das copy paste ich
mir mal wieder zusammen, decode and resize init, wisst ihr was, ich machs, ich mach das
hier, ich find das hier an der Stelle besser zu lesen, weil hier frag ichs auch ab, so,
ok, lass mal überlegen, was, was brauch ich jetzt, also ich brauch nen Frame zum decoden,
jetzt brauch ich nen Package, irgendwie sowas wie, keine Out Package, immer, muss man mal
schauen, was wir damit machen, so, also, jetzt muss ich das Bild erstmal decoden, was von
der Kamera kommt, sprich, das ist FFmpeg, das wird auch mal richtig schön copy pasted,
excellent, so, also, send Package an den Decoder, dann decoder das für mich, und hier bekomm
ich das decodete Bild wieder, sehr gut, speichern übrigens dann den Frame, Frame Source, das
heißt, ich muss hier unten schon mal eine Sache machen, FFmpeg, Free Frame, nee, es
ist Frame Free, genau, Frame Free, und zwar, Frame Source, alles Benahmung, ja, oh, das
ist ja, das ist cringe, ey, wenn ich das im Chat lese, ey, it's Knoppers Time, ach,
du Scheiße, wer sowas, ey, wer den Source Code rein schreibt, gut, ich mein, ich darf
nix sagen, ich schreib bei uns manchmal auch so wirklich Sachen wie Poggers und so rein,
aber das versteht außer mir, glaub ich, keiner, deswegen ist das nicht so wild, Knoppers Time
ist ja Paint's Jam, ja, so, also, jetzt decoden wir das, encoden das, so, und wenn es ein,
ah, jetzt weiß ich, was ich damals gebastelt hab, wenn es ein Fehler ist, wenn es ein Fehler
ist, dann geben wir das leere Package zurück, und Free in den Frame, wir wollen ja keine
Memory Leaks verursachen, so, jetzt müssen wir das Ganze resizen, also, sprich, den,
das Package müssen wir resizen zu 300 mal 300, achso, übrigens, stimmt gar nicht, hier,
Frame, Frame Source, muss ich erwähnen, ne, nicht Resizer, wie heißt das jetzt, Resized,
Kompetenz, Frame Resized, mir fällt jetzt grad nix besseres ein, so, so, jetzt müssen
wir diesen Frame encoden als JPEG, haben die Vorgesetzten doch, aber das juckt doch kein,
achso, du meinst das hier im Chat, ja, das ist keine gute Idee, also, man kann ja, man
kann schon teilweise mal eben paar witzige Sachen reinschreiben, aber irgendwelche Leute
beleidigen in Comments würde ich da wirklich nicht, also, erstens, eh nicht, aber auch
so nicht, so, jetzt kann ich mir das andere wieder zusammencopy paste, so, achso, das
heißt ja noch anders, das haben wir ja so genannt, was ist denn Thumb Packet, was ist
das, sowas hier, aha, alles klar, wunderbar, und jetzt sagen wir dann, Packet gleich, kann
ich das ganze ja zusammenwursten, und zwar, Breite, Breite und Höhe, ah, ok, und jetzt
müssen wir noch ein bisschen aufräumen, FFmpeg, Frame resized, muss ich aufräumen, das ist
normalerweise was, was man in C Sharp nie machen muss, dieses manuelle Memorymanagement, Packet
free, aber wenn man hier unmanaged Zeugs hat, da bleibt dann nix anderes übrig, so, ich
hoffe ich verwursache jetzt keine Memory Leaks, und ich hoffe es, also es kann durchaus sein,
dass es jetzt gleich crashed, ne, gucken wir mal, ok, ich hab gut abgeschrieben, es crashed
nicht, so, das heißt hier bekomme ich jetzt das resized Image, ich kann euch sogar beweisen,
dass das resized und in JPEG komprimiert, das passt mal auf, ehm, wir gucken uns jetzt
nämlich mal den Vergleich an, die Größe des JPEG, des verkleinerten JPEGs Images,
zu ehm, zu der ursprünglichen, zu dem ursprünglichen Full HD Frame, guckt mal, ja, guckt mal, also
der ursprüngliche Frame ist ehm, 46 Kilo Byte und das JPEG Bild ist nur noch 13 Kilo Byte,
also das hat schon funktioniert mit dem resizing und dem ehm, encode, so, jetzt haben wir das,
also das scheint ja auch zu funktionieren, was ich da gebastelt hab, sonst wird's jetzt
nicht, sonst wird's jetzt direkt, direkt abkacken, so und jetzt müssen wir folgendes machen,
das was wir hier mit, sag ich, warum ist die Musik so laut, ne, ist gar nicht so laut,
kommt mir nur so vor, ehm, das was wir hier gemacht haben mit der, mit der Object Detection,
das müssen wir jetzt von Hand machen, also sprich, wir müssen einen Request schicken
an localhost 8080detect, ein Postrequest, mit diesen Parametern drin und den Bild als
Base64encodedes, Base64encodedes String, dieses JPEG Image, so, also, was, VM Settings, was
willst du wissen, eh, if you wanna know the Linux, Windows Manager, it's Manjaro Linux
and eh, E3WM, also ich dachte, du meinst, du machst das mit den Kommandos, weil es auf
Englisch, auf Deutsch nicht versteht, ehm, was ist virtualisiert, Linux oder Windows,
eh, Linux, kommt irgendwann wieder YouTube Stuff, ja, im Dezember, eh, Linux ist virtualisiert,
Windows ist Host unten drunter, guck da, Windows Desktops und hier Linux Desktops, exellent,
so, also, wir haben jetzt das JPEG Image und wir müssen jetzt diesen Request hier schicken
an die I, es ist I3WM, genau, was ist, was hat dir eigentlich geritten das WW zu nennen,
ja, alle anderen Buchstaben kannst du halbwegs so buchstabieren wie im Deutschen, nur W nicht,
sondern eben, ich mein, W, was ist an W so kompliziert, warum muss es gerade W sein,
es trägt dann so Blüten wie, dass es in CS nicht AWP ist, sondern AWP, weil die Leute
im Kopf das nicht hinkriegen und ich vertue mich da auch total oft drinnen, welche Software
benutzt du, das ist vor allem bei Workstation, ok, jetzt machen wir erstmal den AP Aufruf
und dann sehen wir auch gleich schon, ob das Ganze funktioniert und dann können wir auch
gleich überprüfen, ob das mit meinem Hoftor funktioniert, zum Erkennen, also wir müssen
jetzt den Aufruf machen an diese Rest-AP oder an diese JSON-AP, so, wir haben eine HTTP-Klein,
wir müssen erstmal unser JSON zusammenbauen, JObject, so, und zwar, was wir jetzt zusammenbauen
müssen ist folgendes, ich kopiere mir das hier mal rein, dass ich das jetzt, dass ich
abgucken kann, gehst du fort, Moment, doch das ist richtig, brauch ich nicht hier, Application
JSON, hier das da brauch ich, das ist das interessante, der Rest ist nicht so, nicht
so wild, also, also, was brauchen wir denn, JSON, add, was haben wir, Detector Name, Detector
Name ist default, so, JSON, add, was haben wir als nächstes, ok, Sub-OP, Data, doch
das war's schon, mehr brauchen wir gar nicht, easy, easy, also, sagen wir, Data, Data, da
müssen wir jetzt ein bisschen erfinderisch sein, wie, Base64 in System Buffers, was,
Binary Data, und hier, wie konvertiert man in C-Sharp möglichst einfach, Binary zu Base64
String, ok, C-Sharp, Base64 String, wie geht das, Endcode, Endcode, Convert to, excellent,
deswegen mag ich das.NET Framework, für jeden Scheiß irgendwas dabei, also, was war
das jetzt, Convert to Base64 String, Convert, was jetzt, to Base64, das gibt's einfach
schon, nice, so, Byte Array, ist JPEG Image, Data, gut, und jetzt brauchen wir noch eins,
JSON, add, wie heißt das, Detect, und da jetzt ein Stern, nee, nee, oh je, wie geht
denn das jetzt, ich brauch so ein, so ein, Sub, Object, legt man das jetzt ja an, wieso
was, Dynamic, Dynamic, eh, Manually, Manually ist gut, Creating JSON, Creating JSON, mach
dir ne Klasse, die du dann serialisierst, das wird glaub ich zu aufwendig für, ah,
guck mal hier, add, ah, ein zweites Objekt, kann man, kann man nicht sowas hier einfach
machen, J-Object, irgendwie sowas, oh, Moment, what the fuck, J-Object, J-Ob, ich kann das
nicht so schreibe, J-Object, und dann, was soll ich irgendwie reinschreiben, ich mein,
was man machen könnte ist, aber das gefällt mir nicht, sowas hier, ich weiß jetzt grad
echt die Syntax nicht, ich mein sowas könnte man hier machen, Temp, Add, und dann Detect,
ne, Moment, ne, Moment, nicht Detect, Sternchen, Sternchen, mach mal 50 oder 60, ich mein,
so könnte ich das, aber das ist doch mega hässlich oder, ah, das machen wir doch,
das kann man doch nicht machen, iiih, ah, wird ja immer schlimmer, MonkaS, was schlägt
der mir denn hier vor, what, ich könnte mit dem Dictionary machen, dann sag ich, das muss
einfacher gehen, das muss einfacher tun hier, aha, J-Object, Leute, ich hab ne Idee, also,
der kann nämlich das irgendwie, was ist das, das sah ja abartig aus, so, guck mal, der
kann das so, kann ich jetzt nicht einfach sagen, New J-Object, einfach hier reinschreiben,
New J-Object, ist da das, und dann, und dann hier dahinter, und dann hier dahinter schreiben,
so, ob das geht, weißt du, das geht, ich hab da, ich hab da so meine Zeit, aber ich meine,
es gibt keine Syntaxfehler, das heißt, dass, die Chance, dass das geht, ist schon mal relativ
gut, gucken wir mal, lassen wir uns das mal ausgeben, tu String, du brauchst nicht mehr
das zweite J-Object, ja, wie, wie, wie schreib ichs dann, ok, schauen wir mal, ob zumindest,
dass, was sinnvolles dabei rauskommt jetzt, einfach als Dictionary, ah, Moment, ah ja,
Moment, äh, oh je, ich muss ja so machen, New, Detect, das funktioniert doch nicht,
fuck, oh man, ich bin so dumm, irgendwie ein bisschen Jason zusammen zu wursten, das kann
doch nicht sein, was macht das, das New Object noch da weg, ok, so, ne, ne, ne, ne, ne, das
ist ja kein Ruby hier, sowas hier, ne, ne, ne, ach du Scheiße, wie macht man das, ja,
Öffnungsskript Interop, macht ihr doch sonst einfach ein anonymes Object, ne das geht nicht,
siehste doch hier, genau das hab ich doch, das geht nicht, es ist ja kein Ruby und außerdem
in anonymen Objekten es geht ja keine Variable oder keine Property-Names mit Sternchen, ok,
ich, ok, ich muss mir die Hilfe angucken, also, New Object und dann einen NewJ-Property
drin, ich hätt mir einfach die ganze Sache, selbst an, einfach gleich mal die Hilfe angucken,
UJ-Object und dann UJ-Property und dann das da rein easy und jetzt gehts passt auf easy
as fuck ha ha guck mal da funktioniert das ist mein base 64 encodedes build base 64 encodedes
build und die folgt alles klar funktioniert gut ja das ist dann so einfach in den schalter hätte
ich nicht gedacht es gibt wahrscheinlich schönere varianten das zu machen aber es ist ein json
objekt das so formatiert ist wie man das braucht also so und jetzt wird es spannend jetzt gucken
ob die ob die result oder wer braucht das ergebnis ne ne wir brauchen das ergebnis weil da steht
ja dann drinne was das woran was erkannt hat also also also den attp client post und zwar
attp doppelpunkt doppelslash localhost doppelpunkt 80 80 slash die tag so new content wir machen mal
das ganze in die nächste zeile das hier auch new string content hier kommt dann das json rein
muss man das jetzt noch to stringen wahrscheinlich to string encoding ist utf 8 und das ist ein bisschen
also dieser hatte dieser hatte tp handling und json handling finde ich in sich scharf ist auch
ist es zweckmäßig aber schon ist es nett so text ne application json ist es ab application
json so jetzt schicken wir den response dahin alles klar ok
los besser so jetzt lesen wir die response ein
also machen wir das jetzt am besten response ne awaiten müssen wir das ganze await deswegen
haben wir das hier glücklicherweise ssync gemacht oder auch ssync response response wie ging das
jetzt das da was content dann irgendwie read read s string genau auch wieder awaiten text und das
gehen wir mal aus so also sprich von der idee her sollten wir jetzt unser base 64 encodedes
bild an diesen service schicken und als antwort erhalten ob der was erkennt ob der was erkennt
ist fraglich doch mal gucken ob der unseren raspberry pi erkennt mal gucken ob der unseren
raspberry pi erkennt mal hier was bett bett ok es sieht aus wie ein bett aber ja es ist ein bett
sagt er mal gucken ob er meine hand erkennt also er denkt es ist ein bett naja ich mein lasse
ich mal durchgehen das ist eigentlich ist auch ein scheiß bild um da irgendwie objekt erkennung
drauf zu machen wir gucken ob er mein handy erkennt aber das ding ist bestimmt also das ding ist ja
das ding ist ja auf person detection trainiert das ist ja auf also das ist jetzt nicht auf raspberry
pies oder so ok schlägt mich mach mal mein handy mein handy case
ich so wirklich ok ich würde sagen also dieses bild ist nicht so gut dafür geeignet wir machen
mal mein kamerabild was ich mitgebracht habe probieren wir mal aus doch doch bild ist live nur
der process abgekackt also der ist der meinung er erkennt hier irgendwie den bett ok nicht so
ganz aber wenn er meint ja also wie gesagt der ist nicht gut darin sachen zu erkennen wie also
der ist hauptsächlich auf personen trainiert deswegen erkennt er diesen kram auch nicht
richtig der hier also ich glaube das ist hauptsächlich auf personen ok dann machen wir das mal mit einem
kamerabild was ich die ganze zeit schon machen wollte genau ff play flur 2 das hier von meiner
kamera im hof bei nacht was es übrigens gerade hängen geblieben ist warum auch immer also das
nehmen wir jetzt mal als objekt als detection bild und irgendwann kommt ihr auch gleich jemand
durchgelaufen excellent gesagt ich bin es nicht so jetzt jetzt es wird natürlich ein
bisschen kompliziert jetzt müssen wir erst mal diese diese datei die ich hier habe als
rtsp stream bereitstellt um zu faken dass es eine webcam ist das aber glücklicher kein kein
problem weil da habe ich letztens ein sehr nice das projekt gefunden simpel rtsp das da das ist
ein go programm ob es das gibt über gibt es nicht das ist ein go programm und das kann
einfach sachen restreamen also sprich ich kann dateien restreamen als rtsp das ist ultra praktisch
wir laden das ganze jetzt mal runter oder natürlich nachdem es ein go binary ist kannst du ein go
binary auch einfach runterladen und direkt ausführen ziemlich witzlig ist rtsp genau und
schon läuft easy also einfacher könnte es nicht sein rtsp server zu starten als dieses ding doch
das kann vlc aber die syntax von vlc um den rtsp server zu starten ist so wir das kann ich mir
nie merken ok und jetzt müssen wir unsere sache da rein streamen ffmpeg stream 1 file ts was ich noch
dabei ist meistens ist kein audio und meine datei heißt flur 2.ts localhost stream passt gucken wir
mal jetzt bin ich gespannt ob die object detection funktioniert also die person detection
kommt mal er erkennt es da ist einer ins bild gelaufen eine ins bild gelaufen komm her person
person jetzt geht die tür zu und er erkennt wieder nix funktioniert ok lasse gucken wir
doch mal er erkennt nix er erkennt nix er erkennt nix keine detection keine detection doch gleich
geht die kameraloop wieder von vorne los ja wir machen jetzt noch ein kästchen drüber
gleich das wird richtig big brain wir machen gleich noch ein kästchen im frontend drüber
guckt person person person person confidence 79 prozent excellent exquisite nice so und wisst ihr
was das gute daran ist dass die kamera so eine so eine billige china cam motion erkennung wäre
durch die spinnweben schon teilweise angegangen oder durch die bäume im hintergrund und das ding
ist trainiert auf person das heißt du hast viel weniger so falls positive motion detection
so sehr gut sehr gut so als müssen wir noch das ergebnis an den kleinen schicken jetzt
müssen wir noch das ergebnis an den kleinen schicken wie sieht so ein ergebnis aus so gut
das heißt wir machen jetzt mal ein ein muss man kurz gucken wie man das für mal was man
dafür ein ordentliches objekt draus machen dass wir zurückgeben können ok also wir brauchen top
string top fast string top und confidence und label und label confidence ist übrigens ein
double wir machen sie scharp unter linux klar dort net core ist richtig poc das projekt ist
auf github allerdings die objekt detection ist noch nicht dabei weil die objekt detection machen
wir auch gerade das hier ist label label das wird richtig geil wenn das ding hier käschen um die
leute malen kann das wird mega poggers hast du eine zweite auflösungsstufe nee eine zweite
auflösungsstufe bringt nix weil dieses objekt detection api programm ding resized hardcoded
auf 300 mal 300 das heißt wenn ich nicht resize 300 mal 300 resize das ding 300 mal 300 welche
kann kämmen das ist irgendeine china kämmen die ich draußen hängen habe hier die hier warte mal
hier aber stelle die nicht bei amazon da kostet es doppelt wie bei aliexpress 410 irgendwas wehe
oder das teil ist das das ist die kammer aber wie gesagt 60 euro ist eigentlich viel zu teuer
kauft die das woanders bei bengot aliexpress oder irgendwo da bezahlt 30 oder so guck mal
aliexpress gucken wir mal uns den den vergleich an nicht immer aliexpress schwankt immer ein
bisschen der link ops aliexpress fang ich dann immer immer diese üb alda gefahrt oh die gibt's grad gar nicht in 30 euro
rider ja doch guck naja so ein großer unterschied ist aber immerhin immerhin 20
euro billiger als in den d so also top top top bottom bottom right left aber es wird so
nice wenn das kässtchen drum herum malen kann im web interface das ist dann besser als 90 prozent
das ist dann jetzt schon besser als 90 prozent dieser kommerziellen krab video surveillance
dinger ok objekt detektor results das ergibt keinen sinn weil die sachen sind eine detection
drin und schon fortschritte ja person wird erkannt hier person wird erkannt wir gucken
jetzt nur noch wie man das gescheit zum frontend transportieren können um dann kässtchen drum
zu malen ok objekt detektor result wir machen jetzt so was hier objekt detektor result objekt
detektor summary big brain benahmung ich weiß und das ist prop eine liste aus objekt detektor
result die im detections heißt das muss so heißen weil ansonsten klappt es sterilisieren nicht
detektions ok legen wir mal die folgt eine anders wenn die kein exception irgendwie rein
laufen wird das dann so aus wie in china camps ja viel besser viel viel viel mehr rtx on sind
unsere kässtchen so jetzt die serialisen wir das ganze mal und zwar machen wir da jetzt jason
konvert die serialize objekt und zwar ein objekt ok die benahmung benahmung könnte wirklich besser
sein summary und zwar das da wird die serialized excellent so wie nennen wenn das ganze jetzt
richtig konvident das resulten das hier irgendwie response dass man übrigens das bisschen
unterscheide können was was ist so jetzt haben wir das jetzt gucken wir ob in diesem result
detections ob das größer als null ist nur dann hat er was gefunden und wenn das größer als
null ist dann werfen wir unser detected event in walk mit diesem summary trinne eh konvinenz
summary auch so und jetzt kann ich das an den kleinen schicken warum geht das nicht
soll für schmerzen ich habe das falsch genannt moment was macht fu fragezeichenbar das ist auf
null checken also das hier ist das gleiche wie das hier war doch mal vielleicht kann können
die das sogar vielleicht können die das sogar umwandeln für mich da muss ich das nicht schreiben
also das hier ist das gleiche wie if detected und gleich null ist detected also das ist das
gleiche das ist die kurzform davon stba 42 11 monat eins vor hypersubscriber eins vor hypersubscriber
aber dankeschön für den sub also versteht man glaube ich das ist die kurzvariante davon der
checkt ob das null ist nur wenn es null ist macht er das in walk und er bietet mir jetzt auch an
alter komm nimm doch das und das ist das ist wirklich cool an rider dass es so diese kleinen
refactoring sachen drinne hat ok jetzt schickt er mir das zurück jetzt schweißt er mir den event
hier alter ich hab was gefunden mit den mit den werten hier drinne und jetzt muss ich das nur noch
zum klein also nur noch einfach nur zum klein zurückschicken und das kann ich folgendermaßen
ich hab doch hier sogar schon ok exzellent das ist die verbindung zu meinem zu meinen zu meinem
webfront end send user clients all send async so wir brauchen jetzt was was wir aufrufen auf
dem klein zu als zum ersten mal wo wir heute beim klein was machen ok also ist signal rr wie dem auch
sei und detected wenn wir das jetzt einfach auch mal detected und zwar kriegt das dann übergeben
die kamera wofür es ist nicht die kanne sondern die kamera und das summary die berechnung bitte
dann c++ web assembly exzellent was ne das ist javascript das frontend gedöns hier alles alles
in line in einer index html datei wie ist denn jetzt eigentlich der aktuelle javas javascript
style guide leute macht man jetzt semi-colon oder nicht das ändert sich ja alle mal paar
jahre je nachdem was der aktuelle oberjavascript guru sagt macht man jetzt semi-colon am ende
oder nicht in javascript macht man ok weil von pan ja von paar jahren hieß es noch hieß es noch
nein macht man nicht so also und detected könnte man zwar ein bisschen schöner nehmen aber es
egal ok und detected dann schicke ich zurück die kammer halter ich war was habe ich mit meiner
kanne die kamera schicke ich zurück und das summary und das schöne ist an die heißt ja ist
noch nicht so alt das schöne daran ist der macht das ganze serialize für mich also sprich ich muss
hier nichts umwandeln in irgendwelche chasens ist eigentlich ziemlich gut so und ich sollte
jetzt hier was bekommen jedes mal wenn er war wenn er nur eine person ist das so fett
ist das so ultra fett
ja ohne das ist ja huge vielleicht habe ich das mal umgestellt
wir waren zu spät die person wurde gerade detected ID ja gut das ist die kamera die
das schickt aber ich habe gar nicht im lock drinne stehen weit hier lock camp und lock summary so
jetzt warten wir noch ob wir jetzt was zum frontend geschickt kriegen wo er was erkannt hat
postchamp postchamp postchamp postchamp detection beim er erkennt es in der kamera
ID 1 wohnzimmer und hat detected diese koordinaten als person exzellent extrem exzellent so was ist
wenn dann dort ein plumentopf steht erkennt er das als festen gegenstand das ist ja gerade
das gute der unterschied von quasi so das simplen motion detector sache das ist ein
nee das ist das ist eine aufnahme von meiner kamera ja das ist das war gestern abend oder
so meine live bild jetzt ist es eine loop die ich abspiele im hintergrund zum ausprobieren
von diesem ganzen zeug das ist ja der gründer der unterschied dazu herz 93 das ding ist
halt ai sagen wir mal ai mäßig trainiert auf erkennen von leuten das heißt das ist
viel viel weniger fehler anfällig als so eine einfache erkennung mit wie viel pixel
verändert sich im bild das heißt du hast du hast so gut wie keine fehlarme mehr klar
ein paar mal er kennt das trotzdem schrott ja aber du siehst ja es erkennt wirklich
nichts erkennt wirklich nur wenn hier einer durchläuft hier kuck jetzt hat er wieder
erkannt dass das hier so ok also detected objekt jetzt müssen wir noch kässtchen drum
herum malen chat jetzt kommen meine big brain javascript sachen zum zum tragen wie mal
ich da jetzt am besten in kässtchen kässtchen anhand dieser angaben die ich da bekomme wie
mache ich überhaupt in kässtchen ok ok ok also wir machen jetzt erst mal dann sie
kann diff diff overlay mit dem flash player genau klas immer mal overlay overlay overlay
ok also sagen wir jetzt mal irgendwie so overlay ja watsch moment so falsche klammern overlay
und dann machen wir jetzt mal äh äh ähm position position muss man glaube ich absolut
stellen dass das überhaupt funktioniert so dann machen wir mal 50 pixel breit dass man
überhaupt was sieht 50 pixel sieht man jetzt schon was am moment live surfer mal anmachen
dass ich refreshen kann exzellente sollte da nicht automatisch refreshen jetzt sollte
da nicht automatisch refreshen um refresh dann automatisch ah jetzt ok alles klar stellst
du den code an der ist schon public zu all halt ohne diese diese motion detector schicht
jetzt dabei weil die machen wir gerade aber das kann ich danach schon pushen ja doch doch
ich kann die koordinaten kreppen ich weiß ja bloß nicht wie ich ein overlay maler gescheit
ok wir machen was also border border style solid border color red hauptsache dass man
mal was sieht ah da ist er ok ok haha sehr gut also wir haben schon mal ne kästchen ähm
zwei oh das ist ja ganz ganz klein äh kann ich da jetzt sagen kann ich jetzt sowas sagen
wie keine ah top kann man kann man das in prozent machen kann man das in prozent machen
geht das top 10 prozent ah fuck das geht auf die ganze seite von der höhe her ah scheiße
moment wie kann ich denn wenn jetzt sagen dass er dass das nur so groß ist wie das
video ach du kacke wie sage ich dem jetzt dass es nur so groß ist wie das video noch
ein diff drum herum noch noch ein extra diff drum herum ne mal diff doch das geht bestimmt
style 100 prozent kein iframe dann iframe sowas machen wir nicht wait max pack es war
doch css magic oder aha excellent ok jetzt habe ich den overlay wo ich über das bild
drüber malen kann ok das war mal so da haben wir jetzt einen roten punkt oben in der ecke
das ist aber egal so jetzt macht man was übrigens das ist nicht der vue js way wie ich das
hier mache mit ids und so ne das gehört sich so gar nicht das macht man über data text
ja einfach nur mal gucken ob es funktioniert so also jetzt müssen wir uns die koordinaten
abgreifen aus der ganzen das heißt wir gehen jetzt hier in unser detect rein so also gucken
wir erste mal ob summary detect detections hat wenn es keine detection hat machen wir
nix ok vorher brauchen wir das ist dieses das jeweilige overlay objekt dafür also irgendwie
gleich document wie habe ich es genannt overlay mit der jeweiligen kamera id das kann ich
mir 1 2 1 so raus crepen so das ist das element so und da gehen wir jetzt rein erstmal lock
let position gleich summary detections 0 wir gehen jetzt einfach mal aus das ist gerade
nur eine person drauf so jetzt gucke ich erst einmal ob es noch funktioniert wenn hier jetzt
jemand durchs bild läuft ob es dann was ausgibt ich war zu langsam wieder war wieder zu langsam
wieso machst du das selbst und benutzt nicht so mein da weil das hat objekt detection ich
habe keine ahnung wie gut die objekt detection von so mein da ist und ich weiß auch nicht
wie gut so mein da als irgendwie überwachungs software ist was sie höchstwahrscheinlich
nicht kann ist latency frei in browser stream das können nämlich so gut wie so gut wie
keine kann das oder was der ursprüngliche grund ist warum ich das überhaupt selbst
gebastelt habe weil latency frei in browser stream kaum eine software kann hier guck position
ok dann holen wir uns mal die koordinaten raus also hier left gleich position ich post
position left dann right top bottom bottom so achso das das ist in prozent von 0 bis
von 0 bis 1 das heißt noch das ganze mal 100 ihr könnt mir ruhig sagen wenn man das
irgendwie javascript mäßig alle schöner machen kann so lock gucken wir uns mal exemplarisch
an ob left oder was ah guck mal hier 61 61 prozent left 61 prozent left heißt hier so
ungefähr das stimmt das stimmt mit objekt destructing wie wie mache ich objekt was
für ein ding objekt objekt destructing denen fällt auch immer was neues eine what
moment ich habe doch kein array das bringt mir gar nichts habe kein array ich habe unterschiedliche
prop das geht damit nicht so ok wie mache ich denn jetzt über z wie mache ich denn
jetzt mit css über javascript die position von dem ding wie geht das jazz dom objekt
position wie macht man sowas stag overflow excellent irgendwas irgendwas halbwegs neues
ja ja wir stimmen zu kannst du den style doch doch man könnte das deutlich mehr vue js
mäßig machen was ist das gibt es nicht so was wie set diff position autoposition adiv
aha sowas hier objekt destructing geht auch mit objekten hier ok aber ich check check
halt nicht ah moment warte mal so so also kann ich sowas hier machen moment testen wir
mal irgendwie was ich habe über das a b c d e f oder so und dann gleich ging das ging
er so aber mit kelly mit mit mit mit mit prak prak prak prakets praxis wie immer genau
was das geht das funst das glaube ich erst wenn ich sehe auf durchs bild auf ich brauche
objekt detection please das funktioniert nicht ist auch wurscht ich habe es mir ich
habe es mir so rausgeholt jetzt alles alles gut oder habe ich irgendwas anderes kaputt
gemacht links rechts oben unten label confidence ah da macht nicht der reinfolge nach da macht
das nicht der reinfolge nach das muss dann auch wirklich so heißen ach scheiß drauf
so geht es doch auch so die viel interessantere frage war doch wie kann ich das hier setzen
ok also position style ok wobei doch ich finde das cool das machen wir jetzt doch noch war
was man muss das man muss das wirklich mit kelly machen ok wir probieren das jetzt aus
der chat chat kennt sich aus echt krass das funst es funktioniert wobei mir das nix bringt
ich muss es ja noch ich muss es ich muss ja noch mal 100 rechnen mal 100 kann ich hier
drin gar nicht rechnen also ich muss es doch so machen so aber jetzt viel interessanter
also wie ging das andere jetzt el style style so ok ach jetzt wirds big boy left gleich
wie war das hier nochmal ncs so ne left und dann prozent ich hab ehrlich legit kein plan
ob man das setzen kann in java script wir reloaden mal schnell weil er hat ach guck
leute es stimmt die position ist ziemlich ziemlich ziemlich gut ok weiter gehts right
einfach ne moment right geht nicht so right muss ich ausrechnen aber top kann ich noch
so und so machen top aber top wissen wir schon oben guck mal hier das wird das wird leute
es wird es wird das ist schon ziemlich nah dran ok jetzt kommt das nächste el style style
so jetzt wirds ein bisschen komplizierter rechts den rechts ist dann left plus left plus right
gibt das sinn übrigens ist auch nicht right es ist es ist es ist breite ne das ergibt
keinen sinn was ich hier mache oder kann oder muss man oder kann man das so setzen kann
man das einfach so setzen ne ne ne genau es ist ne ne es ist right minus left und dann
muss es breite sein right minus left so und dann höhe höhe ist dann bottom minus top
so und passt auf leute jetzt gehts jetzt gehts easy easy mit ansagen gehts jetzt easy easy
ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha
guckt euch an hier wie wie die profis hier backbend ok das muss ich jetzt nochmal angucken
aber es verschwindet noch nicht ok das kann das kann man gleich noch fixen dass es nicht
verschwindet es sieht wirklich aus wie ein wallhack
wack incoming ok er erkennts hier unten irgendwie aber das liegt daran weil sie erst ins bild
hier rein läuft nice exquisite exellent das stacheldraht ne haust du drauf dass das stacheldraht
ist äh das sind irgendwelche spinnenweben würd ich sagen ja ultra nice ultra nice
exellent exellent so können wir jetzt nicht einfach sowas hier machen set timeout set
timeout und dann keine ahnung nach 2 sekunden nach 2 sekunden machen wir dann el style display
irgendwie none oder so none muss doch funktionieren dann solls nach 2 sekunden wieder verschwinden
wobei das nicht richtig funktioniert weil sich das dann gegenseitig resettet der erste
tickt und resettet während der zweite angezeigt wird ich glaub das ich glaub das funktioniert
nicht so ganz
wie kann ich denn jetzt an die box was dran schreiben content css content oder so ja guck
das blendet sich jetzt gegenseitig aus
woop oh ah kann ich sowas hier machen el style content person geht das so willst du
die objekterkennung wirklich auf raspberry pile auflassen ja erstmal schon je nachdem
wie das performance mäßig funktioniert aber ich denke alle 2 sekunden in frame decoden
und detection machen kann er ok ne das funktioniert so schonmal nicht inner text inner text
die musik aus das bin ich schon so lange on
inner text mal gucken ob das jetzt funktioniert chat wir müssen jetzt leider immer kurz warten
oh person guck mal da steht person man siehts zwar nicht aber es steht dran wahnsinn ich
bin höchst höchst begeistert gibt es nicht sowas wie outer text
kann ich kann ich hier kann ich hier drinne sagen color irgendwie auch red warum lässt
du die box nicht nur anzeigen wenn er auf was erkannte ja das ist feintuning gezeugt
ich bin mir jetzt erstmal schon happy dass überhaupt was anstreicht
genau das wir machen noch hier guck mal person excellent display nun
jetzt ist es weg und erst wenn er was erkennt ist es zu spät war erst wenn er was erkennt
dann zeigt er das an wobei moment ich muss ich muss hier noch hin unten ich muss noch
setzen display style display was macht man da block
wie kann ich dem jetzt text oben drüber schreiben wie also da brauche ich jetzt mal eure html
ex exquisite big brain html kenntnisse wie kriege ich die wie kriege ich es hin dass
person oben drüber steht und wie kriege ich es hin dass ich das besser lesbar mache
also mit einem outline oder so kann man text outline machen in html in irgendeiner art
und weise also was wir auf jeden fall schon mal machen können ist text position gibt
sowas text text position sowas doch text position so sowas gibt es nicht noch ein diff mit schwarz
im hintergrund die text text allein kann ich nur links rechts oben unten und sowas machen
aber nicht wo es wo es steht
den text in ein diff ja dann kann ich da aber nicht so ohne weiteres drauf zu also wir machen
jetzt erstmal ein oder hier font family was sind die areale oder so irgendwas in der richtung
font size 20 pixel dann sieht man schon ein bisschen mehr font emphasis ne aber was ist
zum was ist zum fett machen noch mal alter was ist zum fett norm font weight bold ok
es sieht schon es sieht es sieht immer immer besser aus ja so dann können wir border border
radius aber zwei pixel dann ist es sogar für die apple fans erträglich weil es abgerundet
ist moment es gab subs habe ich gar nicht übersehen set pudding hat was verschenkt
an forza sebastian stufe 1 dankeschön für den sub huge brain sub nice sorry falls ich
heute irgendwelche übersehen habe ich war gerade am konzentraten ok das sieht das sieht
doch jetzt eigentlich schon halbwegs passabel aus guck mal der resize sogar wenn er das
besser erkennt richtig gut richtig gut so ich würde jetzt nur gerne den font ok das
mal wir machen jetzt nicht inner text jetzt habe eine idee ich habe eine idee wir machen
nicht inner text wir machen inner html und da machen wir jetzt p oder so was hat glaube
ich auch einer im chat kann vorgeschlagen p und da machen wir jetzt style oh jetzt wird
aber ich style wie positionieren das jetzt kann ich kann ich kann ich sowas machen wie
irgendwie padding padding top minus 20 oder so person transform translate jetzt wirds
jetzt wirds big brain moment das muss ich jetzt mal kurz kurz versuchen nachzustellen
wie das ne das funktioniert ok moment das muss ich jetzt mal das muss ich jetzt mal
kurz ausprobieren also wir machen hier mal das display nun weg jetzt jetzt tun wir mal
so als machen wir das wirklich also da drinne machen wir jetzt ein p person und das will
ich style kann ich einfach so was sagen wie top minus 20 pixel ne fuck position ne
ah doch ah ne das geht ja aus der box raus
eh promise 1 wenn man das html richtig macht dann ist das kein ding
wie kann ich das überhaupt fix kann ich bestimmt mal sowas hier
papa Leute ich habe es ich habe es excellent minus 20 dass smaller jetzt ja messes tritts
Ach, jetzt bewegt sich's. Fuck, jetzt bewegt sich's nicht.
Ah!
Nur mit Transform. Okay, okay, ich mach das mit Transform.
Wenn du das sagst, ich mach das mit Transform.
Also, was muss ich machen? Transform.
Transform, Translate.
So.
Was? Minus 100? Minus 10?
Ey, das klippt nicht. Das bleibt in der Box drin.
Glaub ich zumindest, oder?
100?
Okay, ich glaub dir das jetzt. Ich mach das jetzt einfach mal.
Manchmal, Leute, manchmal muss man dem Chat einfach vertrauen.
Wobei das oftmals auch ganz schnell nach hinten losgehen kann.
Aber der Chat, hier ist ja der Cute Chat, da passiert sowas nicht.
Okay, jetzt mal gucken, wie es jetzt funzt.
Oder auch nicht.
Auf, lauf ins Bild.
Ich sollte den Loop kürzer machen, da schläfst du ja ein, weil.
Ah!
Pog.
Geht einfach. Ist es nicht hübsch? Ist es nicht hübsch?
Kann man, kann man in CSS Outlines machen?
CSS Outline Texts? Geht das irgendwie?
Adding Stroke to Web Texts.
Oh, oh, das ist irgendeine Firefox, Firefox, äh, nee, Moment, Webkit, irgendeine, äh, äh,
Moment, ich bring's durcheinander. Äh, Chrome, ne, bin verwirrt. Safari?
Also Mozilla ist Geckro. Safari ist Webkit, ne? Und Chrome ist Plink, aber Plink war mal Webkit, oder wie war das?
KHTML ist, glaub ich, Webkit. Ich, warte mal, ah, guck mal, wir haben ja Support. Was, was juckt mich, was juckt mich denn, bitteschön, der IE-Support?
IE-Support, was ist das?
Keck, Keck, da reicht Keck W nicht mehr für IE-Support. Das ist Keck, Keck Triple U Extreme.
Keck, Keck, da reicht Keck W nicht mehr für IE-Support. Das ist Keck, Keck Triple U Extreme.
Oh, ich hab's kaputt gemacht, Leute. Was hab ich verkehrt gemacht? Unexpected Token Doppelpunkt.
Warum? Ah, Kacknup. Das brauchen wir noch. Kann man da Enter drücken drinnen und da macht es
trotzdem auf die nächste Zeit. Kann man, kann man sowas machen in JavaScript irgendwie so?
Das war zwar nicht schön, aber man erkennt wenigstens ein bisschen was.
Boah, Boah. Paar Gas hier. Jetzt kann man sogar lesen.
Moment. Das ist aber gar nicht das, was ich haben will. Wartet mal. Text Fill Color Red, Outline White.
Das ist doch schon mal, das geht in die richtige Richtung. Okay, ähm, zwei Pixel dick. Es ist
halt mega hässlich, was ich hier zusammengestrickt habe gerade. Aber es geht ja nur ums Ausprobieren.
Das machen wir dann. Das werden wir auf jeden Fall alles ein bisschen noch Vue.js mäßig
integrieren, weil das man, das ist ja absolut anti Vue.js mäßig, dass man hier irgendwelche
HTML Elemente. Ähm, Face Recognition. Oh, das wird ein bisschen komplizierter. So anspruchsvoll
bin ich gar nicht, was das anbelangt. Äh, was ist das? Iget. Wir machen Mono Space Fun.
Das ist bestimmt viel besser. Mono, Mono Space Fun. So. Macht es ein bisschen heller. Okay,
ähm. Was war mal ein bisschen heller? Also RGB. Hatte Visual Studio nicht sowas, wo
man das hier auswählen kann? Gab es da nicht so ein Track and Drop irgendwie? Konnte das
das nicht irgendwie? Ich bin eigentlich ziemlich sicher, dass es das konnte. Ah, geht doch
hier. So, das ist volle Pulle, volle Pulle rot. Wir machen es jetzt einfach ein bisschen,
ein bisschen heller. Oder? Nee, wir machen. Äh, das ist schon okay. Aber ich, okay, ich
bin, für das Outline bin ich zu dumm. Das kenne ich nicht gebacken. Wie man das schön
macht. Da bin ich zu Low-Brain-Führer. Aber noch mal hier, Outline 1. Aber es ist doch
schon mal was. Ich meine, es highlightet, wo was ist. So, und jetzt müssen wir noch,
jetzt müssen wir übrigens noch probieren, ob das auch mit Mehrfachen kam. Ah, also.
Ah, ja. Also, schön ist was anderes, ja. Schön ist was anderes. Aber wir müssen noch
was anderes probieren, und zwar, ob mehrere Kameras gehen. Okay, wir probieren erst mal
was aus. Ich mache jetzt hier noch einmal die Kamera vom iPhone ran. So, keine Ahnung.
Und, ähm, wir versuchen mal, ob ich das Ding dazu überredet kriege, irgendwas zu erkennen.
Was Red und White vertauschen? Also, du meinst, dass wir hier sagen. Ups. Du meinst, dass
wir hier sagen. White. Und hier sagen Red. Und das heißt, wir müssen hier auch sagen
White oder wie? Äh. Er erkennt ein Bett. Also, dass da Person dran steht, stimmt ja nicht.
Person haben wir ja hardcoded. Das ist auch noch eine Sache, was vielleicht ein bisschen,
was vielleicht ein bisschen verbugt ist, dass wir hier nicht Person reinschreiben sollten,
sondern, ähm, sondern wir sollten reinschreiben Summary Detection. Post. Okay, Post. Label.
Bett. Guck, er erkennt ein Bett. Eindeutig. Eindeutig ist das hier ein Bett. Also, ich
finde, offensichtlicher könnte das gar kein Bett sein. Aber wisst ihr was richtig Geiles?
Es funktioniert auf beiden Kameras parallel schon. So, was brauchst denn? Ich mach das
jetzt nur mit einem grafischen Programm, dass man es ordentlich overlayen kann. Norm System
Monitor. Weil so ein Terminal halt ein bisschen kacke als Overlay aussieht. Monarch S. Guck
mal, das braucht kaum CPU. YouTube Videos zu konvertieren ist gar nicht so einfach in
Sachen, die der Browser abspielen kann über WebRTC. Wie popst du die Fenster aus? Windows
Shift, Leertaste. Okay, das ist ein Bett. Das ist schon mal ziemlich fein. Ich mach mal
kurz den Raspberry Pi weg. Guck mal, ob er es immer noch als Bett erkennt. Jetzt haben
wir noch ein Bett. Es sieht auch echt aus wie ein Bett. Jetzt mal ohne Scheiß. Versetzt
euch mal in diese Detekte. Das sieht aus wie ein Bett. Guck mal, hier oben ist das Kopfstück.
Hier oben ist das Kopfstück. Das hier ist eine Bettdecke und das hier ist das Bett drum
rum. Das sieht wirklich aus wie ein Bett. Also, ich kann schon verstehen, dass das als
Bettdetekt ist. Das ergibt sogar Sinn. Also, das passt schon. Mal gucken, was er mit dem
Tablet macht. Eine Kühlschrank. Geil. Eine Kühlschrank.
Vielleicht hat er auch Probleme mit dieser Beleuchtung. Das ist geil. Wir probieren noch
was aus. Okay. Es ist wie ein Kühlschrank.
Oh, es geht nicht richtig.
Es ist schon mal näher dran als ein Bett. Mal eine Fernbedienung ausprobieren.
Er erkennt da gar nichts mehr. Jetzt erkennt er gar nichts mehr. Probier doch mal das Handy
auf. Lassen.
Ich, ich. Okay, okay, okay. Was?
Das andere erkennt er aber nicht wirklich. Ideen.
Das glaube ich. Das, wo ist der Crap überhaupt? Mal gucken, ob er den ergibt.
Ja, ich glaube, er ergibt den auch, dass da rechts noch so viel anderes Zeug drauf ist.
Ich glaube, das ist der Crap. Ja, das ist der Crap, das ist der Crap.
Okay Leute, vergesst, das klappt nicht. Das klappt nicht.
Aber wohlgemerkt, das Ding ist ja auch auf Person-Erkennung trainiert, ne?
Person, also das erkennt er.
Ja, ich glaube, das ist der Crap, das ist der Crap, das ist der Crap, das ist der Crap.
Dafür ist er nicht trainiert, das zu können. Naja, aber ich muss sagen, doch ein äußerst
erfolgreicher Stream, oder? Ich weiß, es geht einfach. Einfach Bild, einfach Objekterkennung,
ja. Richtig poggers. Äh, übrigens, jaja, als erstes immer Ray steht immer, also, äh,
gute Frage. Vielleicht sollten wir nämlich mal gucken. Du hast Recht, ich nehme ja immer
das erste. Leute, ich nehme immer das erste, was ja im Endeffekt gar nicht mal stimmen
muss. Guck mal hier, ähm, Log, probieren wir das nochmal aus. Also, Summary, ne, Pos,
ne, Summary. Wir geben jetzt einfach mal aus, was er jetzt erkennt nochmal. Wir probieren
das nochmal aus. Ob er überhaupt was erkennt. Okay, das ist jetzt oben die, die Pos, ne,
er erkennt da nix. Der erkennt da nix. Naja, gut. Äh, die G-Streamer-Einstellung, äh,
ne, glaub, die hab ich nirgendswo gezeigt. Naja, gut. Ähm, was ich mich jetzt noch frage
ist, das ist eine Sache, die wir jetzt vielleicht noch kurz machen können. Hat jemand eine
glorreiche Idee, wenn man das hinkriegt, dass das wieder weggeht? Mein Set-Timeout funktioniert
ja nicht richtig. Ich mein, ich könnte sowas machen hier, ähm, keine Ahnung, Set-Timeout,
2 Sekunden, oder 2 Sekunden Timeout. Ähm, If, und dann machen wir so was wie Reset, Reset,
wir machen, wir könnten die Zeit nehmen, irgendwie, irgendwie, keine Ahnung, Set, gleich, äh,
New, oder hier irgendwie Date, Now. Detection-Layer gibt's, äh, Detection-Layer gibt's nicht,
der schickt keine leere Detection zurück. Dann wär das einfach, dann könnte ich Resetten.
Der schickt keine leere Detection, also im Prinzip eigentlich total sinnlos, dass ich
hier überhaupt prüfe, der schickt ja nur, wenn Detection ist, das kann ich mir eigentlich,
das kann ich mir eigentlich, eigentlich gut, dass wir darüber gesprochen haben, das kann
ich mir eigentlich komplett schenken, brauch ich gar nicht. Ne, ne, macht er nicht. Der
schickt nur, wenn was ist. Ich mein, ich hab das ja backhandmäßig selbst programmiert,
also ich weiß, dass der nur was schickt. Guck, das sind jetzt irgendwelche Kameras,
die er schickt, das ist jetzt nicht, guck, es kommt nichts, die ganze Zeit, erst wenn
da was detektet, kommt was. Äh, wäre da nicht Wild True besser? Sorry, bin noob, ja, äh,
in Bezug auf was. Also ich mein, was man machen könnte, ist sowas hier, DateTimeNow, dann
irgendwie, so, also, DateTimeNow, if DateTimeNow minus DateTimeNow größer 2 Sekunden, dann
ElStyleDisplayNone. Ob das, ob das passt, was ich hier gebastelt hab? Achso, Moment, ey,
passt eigentlich. So, geht's jetzt weg?
Äh. Okay.
Was mach ich denn, verkehrt mich jetzt so? Ah, falsch rum, ja klar, es ist DateTimeNow
minus früher, ha, richtig, BigBrainChat, bam, jetzt geht's noch 2 Sekunden weg, jawoll,
du kannst den TimeOut einfach nutzen, okay, jetzt, jetzt komm hier BigBrain, clearTimeOut,
also ich sag einfach hier drinnen immer, clearTimeOut, ich, TimeOut, ich cleare mich da irgendwie
selbst oder so. Ja, aber so funktioniert es doch, aber so funktioniert es doch auch, so
geht's doch auch. Wie gesagt, das ist eben mega dirty gemacht hier gerade, na, Hauptsache,
ich wollt jetzt nur mal, dass man was anzeigen kann. So, wir machen mal ein bisschen länger,
3 Sekunden, 3 Sekunden wird es, glaub ich, frei. Aber das Coole ist, man hat jetzt ziemlich
wenig Fehlalarme. Erkennt er das mit Machine Learning? Ja, das ist trainiert auf irgendeine
Standard Image Database. Komm, wir machen mal die Erkennung runter und schauen, ob er,
ob er vielleicht doch irgendwie den Schraubendreher oder so erkennt. Achso, jetzt muss ich größer
3 machen. Ne, ich muss größer gleich 3 machen. Wir machen mal die Image Detection ein bisschen
weniger aggressiv. Komm, wir gehen mal auf 40% Dings hier, Confidence runter. Wisst
ihr, was wir... Na, okay, okay, okay, okay, na ja, MonkaS. Baseball-Bat. Aber jetzt hat
er 2 Sachen erkannt. Guck mal hier, Baseball-Bat, Baseball-Bat. Na gut, ne gewisse, ne gewisse
Ähnlichkeit ist da. Oh, da oben dreht er ab. Ja, siehste, deswegen hab ich die Confidence
schon ein bisschen höher gestellt. Hund? What? Wo ist dein Hund? Okay, das darf er
nicht machen, 60%. 60% passt schon. 60% ist schon gut, 60% funzt auch. Aber ich finde
das ultra nice, wie verhältnismäßig einfach man das machen kann. Ne, jetzt detaktiert
er nichts mehr, jetzt hat er 60% Confidence. Man muss das auch ein bisschen höher machen,
weil ansonsten erkennt er hier irgendwelche Spinnenweben als Zeug. Wie ist dieses Framework
trainiert worden? Also es ist ja schon komplett fertig, ja, Object Detection Service, der
übrigens auch, glaub ich, eher gedacht ist für Kamerabilder und nicht für so Close-Ups,
wie denkt er halt, es ist ein Baseball-Schläger. Das ist TensorFlow Lite mit, wisst ihr was
wir auch machen, noch machen können, also heute nicht, wir können das über gRPC ansteuern
anstatt über Rest, weil das wahrscheinlich nicht viel Unterschied macht mit diesem kleinen
Spielchen. Es dürfte Performance-mäßig nur wirklich keinen großen Unterschied machen,
denke ich. Die Bilder sind ja nicht groß, das sind, wie groß waren die Bilder? 11 Kilo
Byte, 11 Kilo Byte kann man auch mal in so einem Postrequest schicken. Ja. Aber man
kann es ja mal probieren, da haben wir auf jeden Fall den nächsten Stream schon was
zu tun. Übrigens, nice, wie viele Leute heute am Start sind. Pongas, ich habe selten Programmier-Streams
mit teilweise 240 Zuschauern, also richtig krasser Scheiß hier. Any Primors, nee, nee,
ich bin schon froh über jeden, der da ist. Ich finde es übrigens allgemein, da muss ich
den Chat ja mal, ja, ohne zu sehr rumzuschleimen und ich sage, ich habe wirklich, und das meine
ich jetzt ernst, das ist nicht diese übliche Geschichte mit irgendwie, ah, ich habe die
besten Zuschauer und so, mein Chat ist wirklich extrem angenehm. Vergleiche das mal mit anderen
Chats, da ist im Prinzip zwei lustige Copypastas am Start, aber ich meine, überlegt euch
mal, hier helfen sich sogar die Leute untereinander. Ich habe im Chat schon so viele Fragen gesehen,
wo dann einer darauf geantwortet hat. Das hast du nirgendswo auf Twitch sonst. Das ist nur
hier im, nur der Cute Chat kann das. Never sub, never donate, du hast, du hast die Dry
Heart Emote vergessen. So, wie hat das hier Person, Person. Also das scheint schon spezialisiert
zu sein auf Personenerkennung. Ja, wie das Ganze trainiert wurde, muss man sich hier mal
durchlesen. TensorFlow Lite, ok. Was für Dinger, Coco SSD, MobileNet, ich habe keine Ahnung,
was das ist. Nee, das ist was anderes, oder? Ich kenne mich mit TensorFlow Machine Learning
Zeug nicht wirklich gut aus, deswegen ist das für mich alles so böhmische Dörfer.
Ja, Kengel, da müsste ich aber irgendeine, irgendeine lernen, wie man gRPC richtig verwendet.
Habe ich noch nicht gemacht. Könnte man aber machen. Ich finde es so geil, Leute. Ich finde
das so geil, wie ihr das erkennt. Ich mache mal kurz hier Fullscreen, dass wir die ganze
Sache mal ordentlich und entsprechend huldigen können. Guckt euch das mal an, wie geil das
funktioniert. Wie geil das funktioniert. Das ist so ein Wahnsinns Pock. Das hätte ich
nicht gedacht, dass das so gut klappt. Also WebSockets in WebAssembly habe ich nie gemacht.
Das Kästchen blinkt wegen dem Timeout. Ja, das geht nach 3 Sekunden weg. Das bin ich
nicht. Mit einem anderen Video. Hast du eins? Wir können das noch mal schnell mit einem
anderen Video testen. Ja, klar. Was haben wir hier? YouTube. Aber YouTube Videos konvertieren
ein Format, was der versteht, ist gar nicht so einfach. YouTube. Surveillance-Test-Video
oder so. Ja, meins ist besser. Das Gute daran ist, dass es nicht sonderlich viel CPU-Last
ist. Ja, aber das handlar dann über numerator, 총itytier. Aber ob du das nenne, die happens
Sibaro, was ist das?
Nee, das bringt jetzt nichts. Wir brauchen da schon ordentliche Aufnahme von draußen.
Okay, wir suchen einfach noch einen Park. Outdoor-Park.
Das ist die Pedobear-Videos hier, was ist da los?
Summer Bench Workout, exakt.
Das ist gut, da können wir sehen, wie die getrackt wird.
Da können wir sehen, wie die getrackt wird durch das Bild, das ist gut.
YouTube-DL
Exit
Das Video ist super und draußen. Real-Life durch den Stich.
Es ist aber nicht so einfach, YouTube-Videos in Format zu konvertieren, was funktioniert, also das kann sein, dass es nicht klappt.
Was nehmen wir denn da jetzt mal? 22?
HVC1
30 FPS
YouTube-DL
Das kann durchaus sein, dass es nicht funktioniert.
What?
Muss ich das updaten?
YouTube-DL
YouTube-DL
YouTube-DL
Nehmen wir mal das kleine.
Das geht.
Okay, das für uns, für Object Detection sollte das ausreichen.
Okay, also das heißt, wir machen jetzt FFmpeg.
Ne, eigentlich, eigentlich können wir das, wir müssen es aber umbenennen.
Ich bin mir nicht sicher, ob das das einfach so frisst, ehrlich gesagt.
Benen wir den Kram mal und jetzt palten wir das mal rein.
Publish
Ne, der frisst das YouTube-Video nicht.
Das muss man jetzt in irgendein anderes Format konvertieren, dass er das kapiert.
Wie, wie, also in welches Format kann ich das konvertieren, dass es das kapiert?
Dass ich das verwenden kann.
Seit wann streamst du hier? Seit zwei Jahren oder so?
FFmpeg
Okay, jetzt reencode der erstmal, aber es geht schnell.
Ne, oder? Der reencode klappt gar nicht.
Ach, Moment, das ist
Moment mal, ist das jetzt ein H2-64-Stream?
Das war vorher ein Mpeg-2, ja, kann man ja nicht wundern, dass das nicht funktioniert hat.
FFprobe?
Ach ne, FFprobe-Floor?
Ah, MonkaS.
Wie kriege ich das jetzt konvertiert?
Minus C, copy, no audio, okay, okay, klubts.
Ja, ich hab keine Ahnung, welches Format ich das konvertieren muss, ehrlich gesagt, dass es funktioniert.
Ähm, also die normalen Videos sind problematisch. Oh, es geht, es geht einfach.
Bensch, oh, guck mal.
Person.
Oh, jetzt spackt er rum, Alter.
Meine Detection ist kaputt.
Und, und, ich hab, ich hab viel zu viele Keyframes, 95% CPU-Last, MonkaS.
Das ist mein Browser, der rumspackt, Alter, ich muss, oder ist das das Ding?
Kann es sein, dass dieses Video ultra viele Keyframes hat?
Ja, das Video ist nicht so detailliert, ich glaube, das Video hat zu viele Keyframes.
Oder auch nicht.
Okay, ne, das ist, das ist das Frontend, was rumspackt.
Alles klar, das ist das Frontend, was rumspackt.
Nicht, nicht das Backend, okay, ähm, äh, ich muss das mit dem Timeout kurz wegmachen.
Prom.
Pluton plant.
Jetzt geht's.
Komm mal, her.
Ja, hier ist er, hier ist er.
Ja, ihr ihr auch.
Guck mal hier, wie gut das funktioniert.
Es lag ein bisschen hinterher, aber ist das nicht richtig poggers?
Guck mal hier, wie es trackt.
Mensch, das ist ja richtig geil.
Okay, bewegt er sich noch ein bisschen mehr als da...
Also ich meine, so über das Bild nicht die Bank hoch und runter, dass man mal was anderes
sieht, wie das trackt.
Okay, die Person hört angeblich...
Guck mal, er erkennt es.
Ich muss mal kurz gucken, wie viel Auslastung das Ding hat.
Oh, jetzt wird gejumpt.
Jetzt wird übelst gejumpt, Honor.
Ja, das mache ich auch morgens immer.
Guck mal, nur 7% CPU-Last.
Hat er keinen Track erkennt?
Oh, jetzt wird es aber anstößig langsam.
Aber wie gut er das erkennt.
Guck mal hier, sogar die Umrisse stimmen halbwegs.
Guck mal hier, das erkennt sogar, dass die Grillgestütze zu macht.
Ich meine, das ist auch ein Vorwand, sich Hottegymnastikgrills anzugucken jetzt, aber
wie gut das klappt, ultra nah ist.
Guck mal hier, wie gut er erkennt sogar, also die Umrisse sind ziemlich gut, müsst ihr
mal sagen.
Chat.
So, man treibt es nicht.
Okay, ich hoffe ja, dass die sich noch ein bisschen über das Bild irgendwie bewegt, dass
man auch erkennt, dass das in der Bewegung gut funktioniert.
Was machst du mit dem Tool?
Rumspielen erstmal.
Guck mal, und das alles mit 6% CPU-Auslastung.
Guck mal, wie es wieder erkannt hat, dass es jetzt steht.
Ich meine, es ist ein Keyframe hinterher, also es ist klar, ich meine, es extractet
nur jeden Keyframe, das heißt, es leckt ein bisschen hinterher bis zum nächsten Keyframe,
aber who cares, dafür braucht es kaum CPU-Auslastung.
Sehr nice.
So, gibt es noch irgendwelche Gymnastik-Videos, wo wir mal gucken können?
Oder, ich gucke mal, ob die sich später noch ein bisschen bewegt hier, nee, nee, nee,
das...
Aber doch, hier, am Ende, am Ende läuft es noch ein bisschen rum.
Okay.
Outdoor.
Sag mal, was sind das alles für Videos, Alter?
Überall die Hot'n'Grills bei irgendwelchen Workouts.
Okay, der bewegt sich bestimmt ein bisschen mehr jetzt.
Nee.
Ah, doch.
Excellent, der bewegt sich ein bisschen mehr.
Okay, probieren wir das mit dem Video noch mal aus.
Ey, diese Fitness-Videos sind optimal, um so was zu testen.
Da bewegen sich Leute drauf und das ist genau das, was man brennt.
Personen im Hintergrund, gut, ich nehme aktuell immer nur das erste, was matcht.
Stimmt, bei zu vielen Personen, ich matche auch aktuell nur eins.
So, was nehmen wir, 18, 18, was, nee, oh, okay, also ich loop zwei, Punkt, mp4, und
jetzt müssen wir das Ganze konvertieren, loop zwei, Punkt, loop zwei, beste, beste Namen
aller Zeiten.
So, und jetzt füttern wir das hier rein, excellent, loop zwei.
Es kann sein, dass er das Video nicht frisst, wie gesagt, Videos von YouTube, ah doch, geht,
easy.
Hier, Person, er kennt doch, und das alles bei fast kaum CPU-Last, nee, zwei Kästen zusammen
kann er nicht, weil wir nur ein Overlay-Element haben.
Also, es kann durchaus sein, dass er theoretisch die Kinder im Hintergrund erkennt, aber der
nimmt immer nur das, was er am höchsten erkennt, als erstes.
Das läuft immer Machine Learning, ja, komm mal hier, er hat gleich erkannt, bäm, dahin
steht er.
Ich bin echt, ihr merkt vielleicht, ich bin wirklich extrem begeistert, wie gut das geht.
Mein erstes AI-Einsatzgebiet bisher, ich habe ja noch nie nichts AI-mäßiges bisher
verwendet.
Na okay, das kriegt er nicht hin, ist ja nur ein Keyframe alle zwei Sekunden oder so.
Kannst du die erkannten Keyframes auch als Standbild exportieren, um dann zum Beispiel
an Smartphones zu schicken?
Theoretisch kann ich das machen.
Notifications, muss ich mir dann irgendwie überlegen, wie man das einbaut.
Als nächstes werde ich jetzt erstmal einbauen, dass der automatisch was aufnehmen kann, wenn
er was erkennt.
Also ich werde es so machen, der zeichnet permanent 10 Sekunden auf und sobald es einen
Aufnahme-Event gibt, das kann entweder ein Klick im Web-Interface sein oder Motion-Erkennung
oder eine gewisse Zeit, dann nimmt er auf und hängt aber die letzten 10 Sekunden an
die Aufnahme mit dran, weil ansonsten hast du das Problem, wenn es Motion-Erkennung macht
und die Person übelschnell rennt, dann kriegst du es nicht mit.
Deswegen nehme ich immer die letzten 10 Sekunden auf und mache die an der Aufnahme mit dran.
Man sieht jetzt genau, dass da ungefähr zwei Sekunden der Keyframe hinten dran sind, habt
ihr gesehen, Cellphone, exellent, nein ich habe keine Informatik studiert, Cellphone
ist das, ok wusste ich noch gar nicht, aber aktuell ist er der Meinung, Cellphone ist
das höchste, das wichtigste, was er erkannt hat in dem Bild, exellent, ich bin wirklich
zutiefst begeistert, wie gut das funktioniert, aber Leute, ich habe Hunger, ich habe Hunger,
ich hoffe euch hat der Stream gefallen, schön, dass so viele da gewesen sind, wenn ihr noch
Prime übrig habt und mich supporten wollt, wäre das sehr nice, eine Runde Setout muss
sein, ich denke, das war auch mal wirklich ein interessanter Stream, ihr habt mir auch
gesehen, wie viele Diskussionen im Chat waren und wie viele Leute am Start waren, bei dem
Programmier Stream, sehr nice Geschichte, ja und jetzt haben wir Schluss, ich gehe jetzt
was essen, dann gehe ich später eine Stunde oder eineinhalb, ja zwei Stunden gehe ich pennen,
machts gut, wir sehen uns im nächsten Stream, bis dann, see you.
